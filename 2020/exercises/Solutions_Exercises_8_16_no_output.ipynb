{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 - Exercises 8 to 15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8L_jaAr9S8v"
      },
      "source": [
        "# Python Programming for Linguists\r\n",
        "**03 - Python for (Corpus) Linguists**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOAC6yCfxu-f"
      },
      "source": [
        "Downloading (*git cloning*) the workshop repository. The [\"magic command\"](https://ipython.readthedocs.io/en/stable/interactive/magics.html) `%%capture` will suppress any cell output. Be careful: `rm -r python-programming-for-linguists` will delete previous files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mHW5hAMApoA"
      },
      "source": [
        "%%capture\r\n",
        "!rm -r python-programming-for-linguists\r\n",
        "!git clone https://github.com/IngoKl/python-programming-for-linguists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNpGZLOjYPLC"
      },
      "source": [
        "## A. New Syntax and Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfMAgONy5Lc3"
      },
      "source": [
        "We will be using some new syntax and tools for these exercises. Here are some basic examples. Don't worry, these will be used rather lightly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUjmNZbexWjy"
      },
      "source": [
        "### 1. Miscellaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a41_8vw0VyR"
      },
      "source": [
        "##### Lists and Sets\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIuNRycqxlVH"
      },
      "source": [
        "tokens = ['a', 'the', 'car', 'the']\r\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fweVx2qPLalB"
      },
      "source": [
        "types = set(tokens)\r\n",
        "types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgiPlyM0iCJ"
      },
      "source": [
        "##### The `.join()` method (on strings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LG2hP560p3i"
      },
      "source": [
        "tokens = ['The', 'cat', 'is', 'grey']\r\n",
        "s1 = ' '.join(tokens)\r\n",
        "s2 = '-'.join(tokens)\r\n",
        "\r\n",
        "s1, s2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkrmCL580nIs"
      },
      "source": [
        "##### Lambda Functions / Anonymous (nameless) Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1r-wX6u0pOr"
      },
      "source": [
        "x = lambda a: a + 10\r\n",
        "x(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHVcHXMvOTRS"
      },
      "source": [
        "We will be using a Lambda below when using `.apply()` on a DataFrame (see Pandas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjY51inka2Mp"
      },
      "source": [
        "##### `Counter` objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4XdDHoDa4m9"
      },
      "source": [
        "from collections import Counter\r\n",
        "\r\n",
        "numbers = [1, 1, 2, 3, 3, 4]\r\n",
        "counts = Counter(numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE4awcwda-rN"
      },
      "source": [
        "counts[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1H7MDvIbAiw"
      },
      "source": [
        "counts.most_common(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XFF1VEXovLa"
      },
      "source": [
        "##### Adding to Variables\r\n",
        "\r\n",
        "Python supports the `+=`and `-=` operators to easily add or substract from a variable. This also works when concatenating strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBDY_7PoyPB"
      },
      "source": [
        "a = 1\r\n",
        "a += 5\r\n",
        "\r\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kk8tUCFpCGx"
      },
      "source": [
        "b = 'Hello'\r\n",
        "b += 'World'\r\n",
        "\r\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBPImDUqc-9"
      },
      "source": [
        "##### Enumerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rsMqU5YqeIU"
      },
      "source": [
        "l = ['A', 'B', 'C']\r\n",
        "\r\n",
        "for i in l:\r\n",
        "  print(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVE8OW0lqgdO"
      },
      "source": [
        "for e, i in enumerate(l):\r\n",
        "  print(e, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u28uZ4CqzIo"
      },
      "source": [
        "##### Slicing Notation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSlJgKa1rPm1"
      },
      "source": [
        "The syntax is: *start:stop:step* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1554Oi_Gq1VB"
      },
      "source": [
        "l = [0, 1, 2, 3, 4, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0muZDKcmrR_v"
      },
      "source": [
        "l[1:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXmSRb9orUeY"
      },
      "source": [
        "l[0:5:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNeomNi3eXv0"
      },
      "source": [
        "### 2. List Comprehensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3Zn2jdA7u8"
      },
      "source": [
        "numbers = [1, 2, 3]\r\n",
        "n_times_ten = []\r\n",
        "\r\n",
        "for number in numbers:\r\n",
        "  n_times_ten.append(number * 10)\r\n",
        "\r\n",
        "n_times_ten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYI-m9txBOpm"
      },
      "source": [
        "[n * 10 for n in numbers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZbx8EeyBYIv"
      },
      "source": [
        "lol = [\r\n",
        "       [1, 'A'],\r\n",
        "       [2, 'B'],\r\n",
        "       [3, 'C']\r\n",
        "]\r\n",
        "\r\n",
        "lol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA-lNPM6Bi9_"
      },
      "source": [
        "for n in lol:\r\n",
        "  print(n[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuQ8x_1TBvkC"
      },
      "source": [
        "[n[1] for n in lol]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hq2R4Wghrms"
      },
      "source": [
        "### 3. Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwq56INvPd3t"
      },
      "source": [
        "When importing libraries, we can use `as` to give the library another name. For `pandas`, it is convention to simple use `pd` as an alias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTdiV4zYB0-Q"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4HMvNV8CBGn"
      },
      "source": [
        "df = pd.DataFrame()\r\n",
        "\r\n",
        "df['Document'] = [0, 1, 2, 3]\r\n",
        "df['Tokens'] = [1000, 2000, 3000, 3000]\r\n",
        "df['Sentiment'] = [0.2, 0.3, 0.8, None]\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4x6pnfSq7k"
      },
      "source": [
        "Pandas has many methods that help with getting data into your programs. For example, here we are using `read_csv()` to read a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhukaVACHpO"
      },
      "source": [
        "df_2 = pd.read_csv('python-programming-for-linguists/2020/data/numerical/pandas_demo.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObPuSGNCl6A"
      },
      "source": [
        "df = df.set_index('Document')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfNkJIc0C7JS"
      },
      "source": [
        "df['Tokens']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_KUQ2awDCWx"
      },
      "source": [
        "df['Tokens'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3bXcCYyDIKZ"
      },
      "source": [
        "df['Sentiment'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96elnKBzDPrM"
      },
      "source": [
        "df[df['Tokens'] > 2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA4SPFCGSCVc"
      },
      "source": [
        "This selection works based on boolean logic (True/False). `df['Tokens'] > 2000` will return a series of True/False statements for each row in the DataFrame that correspond to the criteria (`> 2000`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzPP--WXSP9K"
      },
      "source": [
        "df['Tokens'] > 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWICqmw3DaQ0"
      },
      "source": [
        "df.fillna(df.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws_LDA_BxRXp"
      },
      "source": [
        "The `.apply()` Method can be used to apply a function to a row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la5sdMgzxQFF"
      },
      "source": [
        "def double(x):\r\n",
        "  '''This function will double a given number.'''\r\n",
        "  return x * 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzLuBhnP1pa"
      },
      "source": [
        "We will `apply` the `double` function to axis 1 (rows). As you can see, all numbers have doubled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67PMBLTmPo4C"
      },
      "source": [
        "df.apply(double, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-hhG8rQBCq"
      },
      "source": [
        "Sometimes we might want to use column values while using apply. Here Lambdas come into play. In the example below, we want to create a new column that contains *Sentiment* times 100. We will be using a very simple function `times100` to do that. In the `.apply()` method, we will be using a Lambda to pass the relevant column (*Sentiment*) to the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1l69GnQXs7"
      },
      "source": [
        "def time100(x):\r\n",
        "  return x * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWZIYqq8QPYj"
      },
      "source": [
        "df['Sx100'] = df.apply(lambda row : time100(row[1]), axis=1)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGM5yrfSYN0E"
      },
      "source": [
        "## B. Exercises (8 to 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpTf9HnDZZBL"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qV_x2ogW9qb"
      },
      "source": [
        "Here, we are setting up our environment. First, we are installing two additional libraries/dependencies - `textdirectory` and `justext`.\r\n",
        "\r\n",
        "Then we are `import`-ing all the needed dependencies.\r\n",
        "\r\n",
        "Finally, we are using two scripts, provided in the repository, to download two corpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoWWCg--DtgW"
      },
      "source": [
        "%%capture\r\n",
        "!pip install textdirectory --upgrade\r\n",
        "!pip install justext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eneM8GARD4Yg"
      },
      "source": [
        "# Basics from Python's standard library\r\n",
        "import re\r\n",
        "import statistics\r\n",
        "import math\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "from operator import itemgetter\r\n",
        "\r\n",
        "from io import StringIO\r\n",
        "\r\n",
        "# Data Science\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# XML\r\n",
        "import lxml\r\n",
        "\r\n",
        "# NLP\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import spacy\r\n",
        "from spacy import displacy\r\n",
        "import textdirectory\r\n",
        "\r\n",
        "# Web\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import justext\r\n",
        "\r\n",
        "# Formatting output\r\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7DAj4DhD-IO"
      },
      "source": [
        "%%capture\r\n",
        "!cd python-programming-for-linguists/2020/data && sh download_hum19uk.sh\r\n",
        "!cd python-programming-for-linguists/2020/data && sh download_coca.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_BfVcYGKLRw"
      },
      "source": [
        "### Exercise 8 – Concordancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zQqd5poEelj"
      },
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txSWdW1MW1Va"
      },
      "source": [
        "We can use `.get_text()` to get the actual text. If the documents/files have not been transformed yet, this will simply load the text from the given file.\r\n",
        "\r\n",
        "Be careful: `.get_text()` can also provide you with texts that are not part of the aggregation (i.e., that have been filtered out)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZd7GGSYE2FX"
      },
      "source": [
        "wikipedia.get_text(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIeAFZPxqVNp"
      },
      "source": [
        "#### RegEx-Based Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol4tTp5WUE8k"
      },
      "source": [
        "It is technically not necessary to `compile` the regular expression. However, it often makes the code more readable and it is also advisable when using the same expression multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkfRfUX0E6vh"
      },
      "source": [
        "cologne = wikipedia.get_text(0)\r\n",
        "regex = re.compile(r'.{0,25}city\\b.{25}|city\\b.{0,25}', re.IGNORECASE)\r\n",
        "concordances = re.findall(regex, cologne)\r\n",
        "\r\n",
        "concordances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDPGIL_lqRck"
      },
      "source": [
        "#### Token-Based Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEydo1FJqdqq"
      },
      "source": [
        "Below we will define a `tokenize` function, which we will use repeatedly. This simple regex tokenizer, despite its simplicity, works quite well for English. Feel free to replace this function with something more powerful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak-pel7GGKbS"
      },
      "source": [
        "def tokenize(text):\r\n",
        "  return re.findall(r'\\w+', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8A43pLVGajC"
      },
      "source": [
        "tokenize('Hello world')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzDTsD7NXGTM"
      },
      "source": [
        "In this variant, we are not differentiating between the left and right span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhfYBNP9Gdiv"
      },
      "source": [
        "cologne_tokenized = tokenize(cologne)\r\n",
        "search_word = 'city'\r\n",
        "lr = 4\r\n",
        "\r\n",
        "for id in range(len(cologne_tokenized)):\r\n",
        "  if cologne_tokenized[id] == search_word:\r\n",
        "    kwic = ' '.join(cologne_tokenized[id - lr : id + lr + 1])\r\n",
        "    print(kwic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5jC3INWXL1L"
      },
      "source": [
        "Here, we are creating two separate strings for the left and right span. These are then printed using `tabulate`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDQyC4ZLH-HP"
      },
      "source": [
        "cologne_tokenized = tokenize(cologne)\r\n",
        "search_word = 'city'\r\n",
        "lr = 4\r\n",
        "kwic = []\r\n",
        "\r\n",
        "for id in range(len(cologne_tokenized)):\r\n",
        "  if cologne_tokenized[id] == search_word:\r\n",
        "\r\n",
        "    l = ' '.join(cologne_tokenized[id - lr:id])\r\n",
        "    r = ' '.join(cologne_tokenized[id + 1: id + lr + 1])\r\n",
        "    kwic.append([l, search_word, r])\r\n",
        "\r\n",
        "print(tabulate(kwic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI56CNlzXV8z"
      },
      "source": [
        "It is very helpful to sort concordances. Given our approach above, we can sort either by the left or right context. We can use `itemgetter` to sort the list of lists based on a subkey."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K10f5ATI0qa"
      },
      "source": [
        "kwic.sort(key=itemgetter(2))\r\n",
        "print(tabulate(kwic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifm5BisVseyt"
      },
      "source": [
        "### Exercise 9 - N-Grams\r\n",
        "Note: Number of N-Grams = Tokens + 1 - N"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obM4_vWMJsNk"
      },
      "source": [
        "text = 'I really like Python, it is pretty awesome.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4oGT1HJyVK"
      },
      "source": [
        "#### NLTP Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGWc-VyXJw5f"
      },
      "source": [
        "def nltk_ngrams(text, n=3):\r\n",
        "  tokenized_text = tokenize(text)\r\n",
        "  ngrams = list(nltk.ngrams(tokenized_text, n))\r\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WywLVUZjKaHx"
      },
      "source": [
        "nltk_ngrams(text, n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgL-zEApKuXI"
      },
      "source": [
        "#### Plain Old Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEdyRRZjKv8v"
      },
      "source": [
        "def ngrams_gop(text, n=3):\r\n",
        "  tokenized_text = tokenize(text)\r\n",
        "  no_of_ngrams = len(tokenized_text) + 1 - n\r\n",
        "  ngrams = []\r\n",
        "\r\n",
        "  for i in range(no_of_ngrams):\r\n",
        "    print(i, tokenized_text[i:i+n])\r\n",
        "    ngrams.append(tokenized_text[i:i+n])\r\n",
        "\r\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVXw0k1-Ljjb"
      },
      "source": [
        "ngrams_gop(text, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbitsVl5NDQF"
      },
      "source": [
        "### Exercise 10 - Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcKmWG4uLx-3"
      },
      "source": [
        "cologne = wikipedia.get_text(0)\r\n",
        "tokenized_text = tokenize(cologne)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kv7q0G1LyRt"
      },
      "source": [
        "#### NLTK Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoQYwE4L57Z"
      },
      "source": [
        "frequencies = nltk.probability.FreqDist(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apj48hltMDzM"
      },
      "source": [
        "frequencies['the']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNTntPAkZgYN"
      },
      "source": [
        "We can easily plot `FreqDist` objects by calling the `.plot()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSPKk2KjMMVG"
      },
      "source": [
        "frequencies.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJmUfTeMZE3"
      },
      "source": [
        "#### Counter Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-sLwsc-MomI"
      },
      "source": [
        "Counter(tokenized_text).most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ4HteNVa--F"
      },
      "source": [
        "#### spaCy Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTGc2UPGbAnh"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "doc = nlp(cologne)\r\n",
        "\r\n",
        "frequencies = doc.count_by(spacy.attrs.IDS['ORTH'])\r\n",
        "\r\n",
        "frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heLgCWHYZ7WQ"
      },
      "source": [
        "If we have the index of a given word (entry in the vocabulary), we can easily retrieve the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-oMpwA6bb7D"
      },
      "source": [
        "doc.vocab[7425985699627899538].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke7zsq6EbjM2"
      },
      "source": [
        "for vocab_index, count in frequencies.items():\r\n",
        "    human_readable = doc.vocab[vocab_index].text\r\n",
        "    print(human_readable, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF9NL_sMl4jQ"
      },
      "source": [
        "### Exercise 11 - Computing Basic Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnFv7TSubhQ5"
      },
      "source": [
        "We use `textdirectory` to load the HUM19UK corpus. Then we are selecting a random sample of 10 texts and transform everything to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOQdtk30NH1a"
      },
      "source": [
        "hum19uk = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/hum19uk', autoload=True)\r\n",
        "hum19uk.filter_by_random_sampling(10)\r\n",
        "hum19uk.stage_transformation(['transformation_lowercase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfF3IY81NdXg"
      },
      "source": [
        "hum19uk.transform_to_memory()\r\n",
        "hum19uk.print_aggregation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT-mZrUWb4u9"
      },
      "source": [
        "For the `get_frequencies` function we are relying on the Counter approach from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv6q6KKCcVE9"
      },
      "source": [
        "#### Basic Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMyHIePjyLrd"
      },
      "source": [
        "Tokenizing in the `get_frequencies` function is convenient for us here. However, this will inevitable lead to us tokenizing some texts more than once - something you would not want to do in a real-life scenario in order to save time and resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_0H0ulNlwL"
      },
      "source": [
        "def get_frequencies(text):\r\n",
        "  tokenized_text = tokenize(text)\r\n",
        "  frequencies = Counter(tokenized_text)\r\n",
        "\r\n",
        "  return frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cDubBTx3Pm-"
      },
      "source": [
        "The `Counter` has a nice additional property. `Counter` objects will return 0 if the element is not present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb2C7KfG3WKP"
      },
      "source": [
        " test_text = 'The cat is black'\r\n",
        " f_cat = get_frequencies(test_text)['cat']\r\n",
        " f_dog = get_frequencies(test_text)['dog']\r\n",
        "\r\n",
        " f_cat, f_dog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSohjZDeN3Mv"
      },
      "source": [
        "def relative_frequency(abs_frequency, no_of_tokens):\r\n",
        "  return (abs_frequency / no_of_tokens) * 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig-_XAwZOI5u"
      },
      "source": [
        "def frequency_across_text(search_term, texts):\r\n",
        "  frequency_list = []\r\n",
        "\r\n",
        "  for text in texts:\r\n",
        "    frequencies = get_frequencies(text)\r\n",
        "    frequency_list.append(frequencies[search_term])\r\n",
        "\r\n",
        "  return frequency_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmGW4KRBcFax"
      },
      "source": [
        "To normalize the frequency counts, we need the number of tokens in the corpus. We can get this number by getting the length (`len`) of the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1tvQaEeP1AK"
      },
      "source": [
        "def frequency_across_text_relative(search_term, texts):\r\n",
        "  frequency_list = []\r\n",
        "\r\n",
        "  for text in texts:\r\n",
        "    frequencies = get_frequencies(text)\r\n",
        "    no_of_tokens = len(tokenize(text))\r\n",
        "    relative_frequency_of_search_term = relative_frequency(frequencies[search_term], no_of_tokens)\r\n",
        "    frequency_list.append(relative_frequency_of_search_term)\r\n",
        "\r\n",
        "  return frequency_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POH6kKAWc7Xx"
      },
      "source": [
        "This list comprehension will generate a list of strings, each containing the text of one document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz149CLIO6Cu"
      },
      "source": [
        "texts = [doc['transformed_text'] for doc in list(hum19uk.get_aggregation())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yq0RIlldBUO"
      },
      "source": [
        "We are now generating the frequencies for *shook* for all texts and storing them in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StPUHEgDOwzP"
      },
      "source": [
        "frequencies_across_texts = frequency_across_text('shook', texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyNRqKAqQW70"
      },
      "source": [
        "frequencies_across_texts_relative = frequency_across_text_relative('shook', texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLjJDgXPZ2o"
      },
      "source": [
        "statistics.mean(frequencies_across_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCHEMuWxPjZM"
      },
      "source": [
        "statistics.stdev(frequencies_across_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWwc2oAzQbcs"
      },
      "source": [
        "statistics.mean(frequencies_across_texts_relative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UF7c3rhQlh6"
      },
      "source": [
        "#### Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chXwPVCGda8V"
      },
      "source": [
        "We typecast (force a new type) the list of tokens into a set. This will remove all duplicates and provide us with an unsorted list of all types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUOs8x5BQvQe"
      },
      "source": [
        "text = hum19uk.aggregate_to_memory()\r\n",
        "tokenized_text = tokenize(text)\r\n",
        "vocabulary = set(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "166s91PjRLj2"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRRsTbtk0Mhf"
      },
      "source": [
        "We could, but here we don't have to, turn this set into a list again. This way, we could order the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fby8RAt30MK0"
      },
      "source": [
        "ordered_vocabulary = list(vocabulary)\r\n",
        "ordered_vocabulary.sort()\r\n",
        "ordered_vocabulary[20000:20010] # Getting a slice of types from the middle of the vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6JM3PEaQnaK"
      },
      "source": [
        "# Initialize the frequency tables\r\n",
        "frequency_table_abs = {}\r\n",
        "frequency_table_rel = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_XaFfcfBSJ"
      },
      "source": [
        "We are looping over the vocabulary (all types in the corpus) and are adding the frequencies (both absolute and relative) to lists. Finally, after finishing a document, we are adding these lists to the frequency tables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7rjEp2XRSoR"
      },
      "source": [
        "for doc in hum19uk.get_aggregation():\r\n",
        "  doc_frequencies = get_frequencies(doc['transformed_text'])\r\n",
        "\r\n",
        "  doc_frequency_list_abs = []\r\n",
        "  doc_frequency_list_rel = []\r\n",
        "\r\n",
        "  for vocab in vocabulary:\r\n",
        "    doc_frequency_list_abs.append(doc_frequencies[vocab])\r\n",
        "    doc_frequency_list_rel.append(relative_frequency(doc_frequencies[vocab], doc['tokens']))\r\n",
        "\r\n",
        "  frequency_table_abs[doc['filename']] = doc_frequency_list_abs\r\n",
        "  frequency_table_rel[doc['filename']] = doc_frequency_list_rel\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy1m9dIYd0Zv"
      },
      "source": [
        "**Absolute Frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB4HcnGHSdOB"
      },
      "source": [
        "df_abs = pd.DataFrame(frequency_table_abs, index=vocabulary)\r\n",
        "df_abs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNmjjN6STyoG"
      },
      "source": [
        "df_abs.loc['the'].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MoOT17Kd2S_"
      },
      "source": [
        "**Relative Frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhmgvd1JUD5T"
      },
      "source": [
        "df_rel = pd.DataFrame(frequency_table_rel, index=vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljsOyt62UK7E"
      },
      "source": [
        "df_rel.loc[['telegraph', 'the']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi088HxRd-En"
      },
      "source": [
        "We sort the DataFrame by its colums before plotting the frequencies for *telegraph*. Since in HUM19UK the files (and so the columns) have years as their names, this will provide us with a diachronic frequency plot.\r\n",
        "\r\n",
        "Of course, this is now based only on our sample of ten. Increase the sample size and run all cells above to get a fuller picture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP-4sUulURz7"
      },
      "source": [
        "df_rel.reindex(sorted(df_rel.columns), axis=1).loc['telegraph'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x2eW9EKeNyx"
      },
      "source": [
        "We can sum up the frequencies across texts for all words. Plotting these, sorted by the total, will result in a (more or less) Zipfian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxoPQh6YU-ic"
      },
      "source": [
        "df_rel['total'] = df_rel.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WefXCriaVF1d"
      },
      "source": [
        "df_rel.sort_values(by='total', ascending=False)['total'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFk4KYuO5NP"
      },
      "source": [
        "### Exercise 12 – NLTK Stemming, Lemmatization, and WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoyBJYtOf1TT"
      },
      "source": [
        "In order to be able to use [WordNet](https://wordnet.princeton.edu), we have to download the database using NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnXcgsGV7yA"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKAFiuWEf_L2"
      },
      "source": [
        "#### Stemming and Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WfVYCxf-Px"
      },
      "source": [
        "Here, we are initializing two stemmers and one lemmatizer. The lemmatizer, as the name suggests, is based on underlying WordNet data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrLxT5XnVl87"
      },
      "source": [
        "porter_stemmer = PorterStemmer()\r\n",
        "lancaster_stemmer = LancasterStemmer()\r\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP9fRqVN-K0q"
      },
      "source": [
        "Please note that there are more stemmers and lemmatizers in NLTK. An interesting one is, for example, the `SnowballStemmer`. *Snowball* is a stemming framework by Martin Porter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrHJXJf7Vtx_"
      },
      "source": [
        "porter_stemmer.stem('connection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaVo9n8WAGu"
      },
      "source": [
        "lancaster_stemmer.stem('connection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XZTDFNVieM"
      },
      "source": [
        "wordnet_lemmatizer.lemmatize('connection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_4xoIpWh6Xr"
      },
      "source": [
        "We can also pass PoS tags to the `WordNetLemmatizer` to make it even better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfceMqYliWaj"
      },
      "source": [
        "wordnet_lemmatizer.lemmatize('driving')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvblX_luh_HA"
      },
      "source": [
        "wordnet_lemmatizer.lemmatize('driving', 'v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCwM9SO6WMWs"
      },
      "source": [
        "words = ['connection', 'become', 'caring', 'are', 'women', 'driving']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEr_LR2JWN5J"
      },
      "source": [
        "for word in words:\r\n",
        "  ps = porter_stemmer.stem(word)\r\n",
        "  ls = lancaster_stemmer.stem(word)\r\n",
        "  wl = wordnet_lemmatizer.lemmatize(word) # We could provide the PoS\r\n",
        "\r\n",
        "  print(f'{word} -  {ps}  {ls}  {wl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-tCNGfgT83"
      },
      "source": [
        "As can be seen above, the three approaches lead to rather different results. The `LancasterStemmer` is the most aggressive but also the fastest of the three.\r\n",
        "\r\n",
        "We can use the magic `%%timeit` command to test how fast these stemmers/lemmatizers work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul5_Md6NWk8w"
      },
      "source": [
        "%%timeit\r\n",
        "porter_stemmer.stem('become')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u4ERoh-Wxwn"
      },
      "source": [
        "%%timeit\r\n",
        "lancaster_stemmer.stem('become')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STamfso4W49J"
      },
      "source": [
        "%%timeit\r\n",
        "wordnet_lemmatizer.lemmatize('become')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MrKAxuzhV6M"
      },
      "source": [
        "If we take the \"best of 3\" metrics, we can clearly see that the, arguably, inferior `LancasterStemmer`can save us a lot of time if we had a very large corpus. \r\n",
        "\r\n",
        "Of course, the lemmatizer was even faster. However, the lemmatizer will only work well if we have data that works nicely with, in this case, *WordNet*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhHIkJhShsYJ"
      },
      "source": [
        "wordnet_lemmatizer.lemmatize('tweets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE0N3oSSgq4z"
      },
      "source": [
        "#### WordNet Synsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-hEwU_zXJUd"
      },
      "source": [
        "search_term = 'fantastic'\r\n",
        "\r\n",
        "for synset in wordnet.synsets(search_term):\r\n",
        "  for name in synset.lemma_names():\r\n",
        "    print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6a71ydw2Dea"
      },
      "source": [
        "### Exercise 13 – spaCy Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_0ijlGYGhl"
      },
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bama5FSyjsVy"
      },
      "source": [
        "For this exercise we are using the smallest (pre-made) model for English available. If you need betters results, you might want to use a larger [model](https://spacy.io/usage/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Ic9SF8YKDr"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "doc = nlp(wikipedia.get_text(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmHiDSyj3zE"
      },
      "source": [
        "#### Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJgbbMOKYvUH"
      },
      "source": [
        "for sent in doc.sents:\r\n",
        "  print(f'{sent}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRiRfShj9jP"
      },
      "source": [
        "#### Tagging / Annotation\r\n",
        "\r\n",
        "spaCy documents consist of tokens. Each token, given the default processing pipeline, also has a lemma, a PoS tag, and its dependencies attached to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPiL6cbKY-Nb"
      },
      "source": [
        "for token in doc[0:10]:\r\n",
        "  print(token.text, token.lemma_, token.tag_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOQZQHUylfiP"
      },
      "source": [
        "We can also loop over all of the named entities. The results here are not great, but this is due to the small model we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8iYAEcXZQEz"
      },
      "source": [
        "for ent in doc.ents[0:20]:\r\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn--IUYXlpHD"
      },
      "source": [
        "#### Dependency Graph\r\n",
        "\r\n",
        "`doc.sents` is a generator. The `next` function will simply provide us with the next available elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUp9Qki9Z-te"
      },
      "source": [
        "sentence = next(doc.sents)\r\n",
        "displacy.render(sentence, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovBYK7RO2GU2"
      },
      "source": [
        "### Exercise 14 - Parsing XML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhBnWOHQbq8P"
      },
      "source": [
        "with open('python-programming-for-linguists/2020/data/xml/bnc_style.xml', 'r') as f:\r\n",
        "  xml = f.read()\r\n",
        "\r\n",
        "xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYHDFoKacAAW"
      },
      "source": [
        "#### RegEx-Based Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8coXLZC9b_6j"
      },
      "source": [
        "def find_elements_re(xml, attribute, att_value):\r\n",
        "  regex = re.compile(f'(<.*{attribute}=\"{att_value}\".*?>(.*)<\\/.*?>)')\r\n",
        "\r\n",
        "  xml_elements = re.findall(regex, xml)\r\n",
        "\r\n",
        "  return [element[1].strip() for element in xml_elements]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cwFCgC_b-Zw"
      },
      "source": [
        "find_elements_re(xml, 'pos', 'VERB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_Dlb6JddEv"
      },
      "source": [
        "#### Parsing Approach (using *LXML*)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VNYeCBedcMT"
      },
      "source": [
        "def find_elements_lxml(xml, attribute, att_value):\r\n",
        "  tree = lxml.etree.parse(StringIO(xml))\r\n",
        "  root = tree.getroot()\r\n",
        "\r\n",
        "  # findall support XPath (see below)\r\n",
        "  elements = root.findall(f\"w[@{attribute}='{att_value}']\")\r\n",
        "\r\n",
        "  for element in elements:\r\n",
        "    print(element.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfL5wCszfDbO"
      },
      "source": [
        "find_elements_lxml(xml, 'pos', 'VERB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u-N6EqgeFOL"
      },
      "source": [
        "##### XPath"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS58s8MoeFBK"
      },
      "source": [
        "tree = lxml.etree.parse('python-programming-for-linguists/2020/data/xml/xpath_example.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M919YiZsAsmD"
      },
      "source": [
        "Get *verbs* on page one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skbQgk__ePKz"
      },
      "source": [
        "elements = tree.findall(f\"/page[@pg_nr='1']/s/w[@pos='verb']\")\r\n",
        "\r\n",
        "[element.text for element in elements]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bsw2xRBAxhF"
      },
      "source": [
        "Get the first word in the second sentence on page two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ534HeEenBp"
      },
      "source": [
        "elements = tree.findall(f\"/page/[@pg_nr='2']/s[2]/w[1]\")\r\n",
        "[element.text for element in elements]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id18_qZ52OyO"
      },
      "source": [
        "### Exercise 15 - Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4OQJTbDfTJx"
      },
      "source": [
        "#### HTML and *BeautifulSoup* Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuLml5O7fRLd"
      },
      "source": [
        "def scrape_wikipedia(url):\r\n",
        "  html = requests.get(url)\r\n",
        "  soup = BeautifulSoup(html.content)\r\n",
        "\r\n",
        "  content = soup.find('div', {'id': 'bodyContent'})\r\n",
        "\r\n",
        "  return content.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p8EWX3-ftQq"
      },
      "source": [
        "scrape_wikipedia('https://en.wikipedia.org/wiki/COVID-19_pandemic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozSrHO97GTcP"
      },
      "source": [
        "Since we are parsing the HTML (similarly to how we used `LXML`), we could also, for example, get all *H2* headlines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1aBcvtrGYz-"
      },
      "source": [
        "html = requests.get('https://en.wikipedia.org/wiki/COVID-19_pandemic')\r\n",
        "soup = BeautifulSoup(html.content)\r\n",
        "h2_headlines = soup.find_all('h2') # This will get all H2 HTML elements\r\n",
        "\r\n",
        "[h2_headline.text for h2_headline in h2_headlines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5aseJ4JgVn2"
      },
      "source": [
        "#### jusText Approach\r\n",
        "\r\n",
        "In the jusText repository you can find a [description of the boilerplate cleaning algorithm](https://github.com/miso-belica/jusText/blob/dev/doc/algorithm.rst)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzcpMxXIgXX1"
      },
      "source": [
        "def scrape_wikipedia_jt(url):\r\n",
        "  html = requests.get(url)\r\n",
        "  paragraphs = justext.justext(html.content, justext.get_stoplist('English'))\r\n",
        "\r\n",
        "  text = []\r\n",
        "\r\n",
        "  for paragraph in paragraphs:\r\n",
        "    if not paragraph.is_boilerplate:\r\n",
        "      text.append(paragraph.text)\r\n",
        "\r\n",
        "  # Combine the paragraphs into one string\r\n",
        "  text = ' '.join(text)\r\n",
        "\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw3CkSdng7tF"
      },
      "source": [
        "scrape_wikipedia_jt('https://en.wikipedia.org/wiki/COVID-19_pandemic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmr79eLxY7-Y"
      },
      "source": [
        "### Exercise 16 - Putting Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sfZtt8uhait"
      },
      "source": [
        "#### 1. Compiling a Tiny Wikipedia Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKw4hrBmhc25"
      },
      "source": [
        "article_urls = [\r\n",
        "                'https://en.wikipedia.org/wiki/Linguistics',\r\n",
        "                'https://en.wikipedia.org/wiki/Sociolinguistics',\r\n",
        "                'https://en.wikipedia.org/wiki/Language_change'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMLjr_HooPU"
      },
      "source": [
        "Since we want all articles in one document (string), we start with an empty string and add the content for each article to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzNk6hAhtjz"
      },
      "source": [
        "wikipedia = ''\r\n",
        "\r\n",
        "for url in article_urls:\r\n",
        "  wikipedia += scrape_wikipedia_jt(url) + '\\n' # Adding a linebreak after each article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RfB2BZpIzT"
      },
      "source": [
        "We are transforming the whole text (corpus) into lowercase; this reduces the amount of types. We are also generating a tokenized version (list) of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l-xpNnmh8Xa"
      },
      "source": [
        "wikipedia = wikipedia.lower()\r\n",
        "wikipedia_tokenized = tokenize(wikipedia)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsnd9C3riBCe"
      },
      "source": [
        "#### 2. Reference Corpus\r\n",
        "\r\n",
        "We are using the COCA sampler as our reference corpus. Since we transformed the target corpus (Wikipedia) to lowercase, we will do the same to the reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqMUwO1viC1c"
      },
      "source": [
        "coca_sampler = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/coca', autoload=True)\r\n",
        "coca_sampler.stage_transformation(['transformation_lowercase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bNXpN70iQrh"
      },
      "source": [
        "reference_corpus = coca_sampler.aggregate_to_memory()\r\n",
        "reference_corpus_tokenized = tokenize(reference_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRX1ZaqiY72"
      },
      "source": [
        "#### 3. Frequency Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDBSl6GRp2Hz"
      },
      "source": [
        "As in Exercise 10, we are getting the vocabulary of both corpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwjaogeAiarc"
      },
      "source": [
        "vocabulary = set(reference_corpus_tokenized + wikipedia_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMNfUK_qxzS"
      },
      "source": [
        "Now, again very similarly to Exercise 10, we can generate a frequency table. We are using `enumerate` to get labels (Target/Wikipedia = 0, Reference/COCA = 1) for the two corpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzkzU4c0ikPL"
      },
      "source": [
        "frequency_table = {}\r\n",
        "\r\n",
        "for i, corpus in enumerate([wikipedia, reference_corpus]):\r\n",
        "  frequency_list = []\r\n",
        "\r\n",
        "  corpus_frequencies = get_frequencies(corpus)\r\n",
        "\r\n",
        "  for vocab in vocabulary:\r\n",
        "    frequency_list.append(corpus_frequencies[vocab])\r\n",
        "\r\n",
        "  frequency_table[i] = frequency_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPc9YkG5jZAs"
      },
      "source": [
        "df_keyness = pd.DataFrame(frequency_table, index=vocabulary)\r\n",
        "df_keyness.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGR3yQa9jw0f"
      },
      "source": [
        "#### 4. Keyness Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGKdU6-5q_Vs"
      },
      "source": [
        "We are using *Kilgariff's Simple Math Parameter* as our keyness statistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6woh-KjwGN"
      },
      "source": [
        "def smp(f_word_c0, f_word_c1, cs0, cs1, k=100):\r\n",
        "  rel_f_word_c0 = relative_frequency(f_word_c0, cs0)\r\n",
        "  rel_f_word_c1 = relative_frequency(f_word_c1, cs1)\r\n",
        "\r\n",
        "  smp = (rel_f_word_c0 + k) / (rel_f_word_c1 + k)\r\n",
        "\r\n",
        "  return smp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P96sVO5Nr3qa"
      },
      "source": [
        "To get some intuition on the SMP, we can have a look at two equally large (1000 tokens) corpora. If the word appears 1000 times in the target and 100 times in the reference, the SMP will be, based on *k*, ten. The *k* parameter works almost as a filter. The lower you set the parameter, the more low-frequency items you will 'get'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ppu9LK5kcr4"
      },
      "source": [
        "smp(1000, 100, 1000, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjA0mRCFlG6Z"
      },
      "source": [
        "df_keyness.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE8Z5BDQsTcw"
      },
      "source": [
        "We can retrieve the corpus sizes by simple checking the length of the token lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4cYMgF6lQFa"
      },
      "source": [
        "cs0 = len(wikipedia_tokenized)\r\n",
        "cs1 = len(reference_corpus_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ab5EwYVtb-C"
      },
      "source": [
        "We can calculate the SMP value for each row (word) by using `.apply` and a Lambda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkMsOUcikpr2"
      },
      "source": [
        "df_keyness['SMP'] = df_keyness.apply(lambda row : smp(row[0], row[1], cs0, cs1), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUrx57HRle4n"
      },
      "source": [
        "df_keyness.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4L5VnD0tUof"
      },
      "source": [
        "In order to get the actual keywords, we can sort the DataFrame by the newly created SMP value and a given cutoff (e.g., 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6s8um65lqZZ"
      },
      "source": [
        "df_keyness[df_keyness['SMP'] > 1.5].sort_values('SMP', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gkUWmaNsiEc"
      },
      "source": [
        "#### Bonus: Stemmed Version\r\n",
        "\r\n",
        "As you can see, in the keyword list we can see that *language* and *languages*, for example, are listed as two keywords. We can use stemming to get a better (well, dependent on your RQ) result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iut_n5qtpRM"
      },
      "source": [
        "This, for the sake of readability and understandability, is just a redefinition of the functions from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz8bEzSytmPg"
      },
      "source": [
        "def smp(f_word_c0, f_word_c1, cs0, cs1, k=100):\r\n",
        "  rel_f_word_c0 = relative_frequency(f_word_c0, cs0)\r\n",
        "  rel_f_word_c1 = relative_frequency(f_word_c1, cs1)\r\n",
        "\r\n",
        "  smp = (rel_f_word_c0 + k) / (rel_f_word_c1 + k)\r\n",
        "\r\n",
        "  return smp\r\n",
        "\r\n",
        "\r\n",
        "def scrape_wikipedia_jt(url):\r\n",
        "  html = requests.get(url)\r\n",
        "  paragraphs = justext.justext(html.content, justext.get_stoplist('English'))\r\n",
        "\r\n",
        "  text = []\r\n",
        "\r\n",
        "  for paragraph in paragraphs:\r\n",
        "    if not paragraph.is_boilerplate:\r\n",
        "      text.append(paragraph.text)\r\n",
        "\r\n",
        "  # Combine the paragraphs into one string\r\n",
        "  text = ' '.join(text)\r\n",
        "\r\n",
        "  return text\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(text):\r\n",
        "  return re.findall(r'\\w+', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWMunzfu-ic"
      },
      "source": [
        "Since we are now stemming our corpus we already have tokenized versions of them. Hence, we do not need/want our `get_frequencies` function to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANCL41VVvH8F"
      },
      "source": [
        "def get_frequencies_tokenized_text(tokenized_text):\r\n",
        "  frequencies = Counter(tokenized_text)\r\n",
        "\r\n",
        "  return frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUbfAXbWuA9r"
      },
      "source": [
        "We need a new function which stems a text (well, a list of tokens). This function takes in a list of tokens and constructs a new list of stemmed tokens using the `LancasterStemmer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tecwh2qqty5y"
      },
      "source": [
        "def stem_tokenized_text(text):\r\n",
        "\r\n",
        "  tokens = []\r\n",
        "\r\n",
        "  for token in text:\r\n",
        "    tokens.append(lancaster_stemmer.stem(token))\r\n",
        "\r\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GcDjFFuuK5e"
      },
      "source": [
        "Of course, we could achieve the same thing using a list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFc6pK1SuUMC"
      },
      "source": [
        "text = 'The cars were driving to through the night.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg7qdil9uKQc"
      },
      "source": [
        "[lancaster_stemmer.stem(token) for token in tokenize(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgT2kYCbswNV"
      },
      "source": [
        "article_urls = [\r\n",
        "                'https://en.wikipedia.org/wiki/Linguistics',\r\n",
        "                'https://en.wikipedia.org/wiki/Sociolinguistics',\r\n",
        "                'https://en.wikipedia.org/wiki/Language_change'\r\n",
        "]\r\n",
        "\r\n",
        "wikipedia = ''\r\n",
        "\r\n",
        "for url in article_urls:\r\n",
        "  wikipedia += scrape_wikipedia_jt(url)\r\n",
        "\r\n",
        "wikipedia = wikipedia.lower()\r\n",
        "wikipedia_tokenized = tokenize(wikipedia)\r\n",
        "wikipedia_stemmed = stem_tokenized_text(wikipedia_tokenized)\r\n",
        "\r\n",
        "coca_sampler = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/coca', autoload=True)\r\n",
        "coca_sampler.stage_transformation(['transformation_lowercase'])\r\n",
        "\r\n",
        "reference_corpus = coca_sampler.aggregate_to_memory()\r\n",
        "reference_corpus_tokenized = tokenize(reference_corpus)\r\n",
        "reference_corpus_stemmed = stem_tokenized_text(reference_corpus_tokenized)\r\n",
        "\r\n",
        "# We need to generate a stemmed version of the vocabulary\r\n",
        "vocabulary = set(wikipedia_stemmed + reference_corpus_stemmed)\r\n",
        "\r\n",
        "frequency_table = {}\r\n",
        "\r\n",
        "for i, corpus in enumerate([wikipedia_stemmed, reference_corpus_stemmed]):\r\n",
        "  frequency_list = []\r\n",
        "\r\n",
        "  # We need to get the frequencies for the stemmed/tokenized version.\r\n",
        "  corpus_frequencies = get_frequencies_tokenized_text(corpus)\r\n",
        "\r\n",
        "  for vocab in vocabulary:\r\n",
        "    frequency_list.append(corpus_frequencies[vocab])\r\n",
        "\r\n",
        "  frequency_table[i] = frequency_list\r\n",
        "\r\n",
        "df_keyness = pd.DataFrame(frequency_table, index=vocabulary)\r\n",
        "\r\n",
        "df_keyness['SMP'] = df_keyness.apply(lambda row : smp(row[0], row[1], cs0, cs1), axis=1)\r\n",
        "\r\n",
        "df_keyness[df_keyness['SMP'] > 1.5].sort_values('SMP', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngmss-RjwKjm"
      },
      "source": [
        "Of course this output is far from pretty (also due to using the relatively fast `LancasterStemmer`). However, it bins linguistic items which belong together.\r\n",
        "\r\n",
        "Also note that this approach does not only work for word frequencies. We could just as well, for example, count PoS tags and look for 'keytags' instead of keywords."
      ]
    }
  ]
}