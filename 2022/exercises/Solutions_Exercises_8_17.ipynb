{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8L_jaAr9S8v"
      },
      "source": [
        "# Python Programming for Linguists\n",
        "**03 - Python for (Corpus) Linguists**\n",
        "as of 2023-01-07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJwE7TYEo1_7"
      },
      "source": [
        "## 1. Environment and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOAC6yCfxu-f"
      },
      "source": [
        "Before we begin, we need to set up **our development environment**.\n",
        "\n",
        "First, we will download (*git cloning*) the workshop repository. The [\"magic command\"](https://ipython.readthedocs.io/en/stable/interactive/magics.html) `%%capture` will suppress any cell output. Be careful: `rm -r python-programming-for-linguists` will delete previous files.\n",
        "\n",
        "\n",
        "Next, we are installing two additional libraries/dependencies: `textdirectory` and `justext`. While many libraries are available on Colab, some need (and can) be installed using `pip`.\n",
        "\n",
        "Then we are `import`-ing all the needed dependencies.\n",
        "\n",
        "Finally, we are using two scripts, provided in the repository, to download two corpora.\n",
        "\n",
        "In addition, we will define a `print_dict` helper function that we will use to look at large dictionaries without breaking *Colab*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mHW5hAMApoA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!rm -r python-programming-for-linguists\n",
        "!git clone https://github.com/IngoKl/python-programming-for-linguists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoWWCg--DtgW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install textdirectory --upgrade\n",
        "!pip install justext\n",
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eneM8GARD4Yg"
      },
      "outputs": [],
      "source": [
        "# Basics from Python's standard library\n",
        "import re\n",
        "import statistics\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "\n",
        "from io import StringIO\n",
        "\n",
        "# Data Science\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# XML\n",
        "import lxml\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import ftfy\n",
        "\n",
        "import textdirectory\n",
        "\n",
        "# Web\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import justext\n",
        "\n",
        "# Formatting output\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeAeFCirn-cW"
      },
      "source": [
        "Downloading two corpora (HUM19UK and COCA sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7DAj4DhD-IO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!cd python-programming-for-linguists/2020/data && sh download_hum19uk.sh\n",
        "!cd python-programming-for-linguists/2020/data && sh download_coca.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJHMf2II4GWY"
      },
      "source": [
        "Helper function for looking at large dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv1PXbG233Kz"
      },
      "outputs": [],
      "source": [
        "def print_dict(d, top=10):\n",
        "  print(list(d.items())[0:top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogbDwP7_jz7w"
      },
      "source": [
        "Here, for convenience, we have a few functions that we are going to use over and over again. \n",
        "\n",
        "Technically, these will be developed over the course of the exercises. However, running this cell makes sure that you don't have to go through every exercise before going back to a specific one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqZmenfukN-l"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  return re.findall(r'[\\w-]+', text)\n",
        "\n",
        "def relative_frequency(abs_frequency, corpus_size):\n",
        "  return (abs_frequency / corpus_size) * 10000\n",
        "\n",
        "def get_frequencies(text):\n",
        "  tokenized_text = tokenize(text)\n",
        "  frequencies = Counter(tokenized_text)\n",
        "\n",
        "  return frequencies\n",
        "\n",
        "def scrape_wikipedia_jt(url):\n",
        "  html = requests.get(url).content\n",
        "  paragraphs = justext.justext(html, justext.get_stoplist('English'))\n",
        "\n",
        "  text = []\n",
        "\n",
        "  for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "      text.append(paragraph.text)\n",
        "\n",
        "  # Combine the paragraphs into one string\n",
        "  text = ' '.join(text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBzngIdcLEeZ"
      },
      "source": [
        "## 2. New Tools and Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb1qBmMkgz4L"
      },
      "source": [
        "### Classes and Objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLAE2PVmJSj5"
      },
      "source": [
        "You can think of classes as blueprints for objects. An object, which is an instantiation of a class, can have attributes and methods (basically functions tied to the object). There's lots more to this, but this should get you going!\n",
        "\n",
        "Here we create a new class `Word`. The class has two attributes (`word` and `length`) as well as one method `reverse`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpnfgklhJGCA"
      },
      "outputs": [],
      "source": [
        "class Word():\n",
        "  \n",
        "  def __init__(self, word):\n",
        "    self.word = word\n",
        "    self.length = len(word)\n",
        "\n",
        "  def reverse(self):\n",
        "    self.word = self.word[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNUoc8LpJ3Jo"
      },
      "outputs": [],
      "source": [
        "new_word = Word('cat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZiwYBoDKFZs"
      },
      "source": [
        "Now we have created a new object based on our blueprint. We can access the instance attributes by using `object.attribute`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOsx-klVJ-Aw"
      },
      "outputs": [],
      "source": [
        "new_word.word, new_word.length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtBzBdJKYMH"
      },
      "source": [
        "Of course, we now also use the methods of the object by calling `object.method()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "015fB5fhKbXC"
      },
      "outputs": [],
      "source": [
        "new_word.reverse()\n",
        "new_word.word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-8lFCNwiEtp"
      },
      "source": [
        "### List Comprehensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G7Z1yWA5tql"
      },
      "outputs": [],
      "source": [
        "numbers = [10, 20, 30]\n",
        "times_ten = [n * 10 for n in numbers]\n",
        "\n",
        "times_ten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORDQhxIF53nm"
      },
      "outputs": [],
      "source": [
        "list_of_lists = [['A', 1], ['B', 2], ['C', 3]]\n",
        "only_first_element = [n[1] for n in list_of_lists]\n",
        "\n",
        "only_first_element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGV6kC8ebGF"
      },
      "source": [
        "### Enumerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zJcldtpedGu"
      },
      "outputs": [],
      "source": [
        "l = ['A', 'B', 'C']\n",
        "\n",
        "for index, value in enumerate(l):\n",
        "  print(index, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siURhumtutW6"
      },
      "source": [
        "### ftfy – Fixing Unicode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW-1rXUcE8NL"
      },
      "source": [
        "`ftfy` by Robyn Speer is an incredibly simple (to use) and useful tool for fixing problems with Unicode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-dLiUGdutp5"
      },
      "outputs": [],
      "source": [
        "unicode_string = 'âœ” No problems'\n",
        "ftfy.fix_text(unicode_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGM5yrfSYN0E"
      },
      "source": [
        "## 3. Exercises (8 to 17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_BfVcYGKLRw"
      },
      "source": [
        "### Exercise 8 – Concordancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTQVw3BqLtSI"
      },
      "source": [
        "#### Corpus / Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K10f5ATI0qa"
      },
      "outputs": [],
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KskKmGJ7Zx25"
      },
      "source": [
        "We can use .get_text() to get the actual text. If the documents/files have not been transformed yet, this will simply load the text from the given file. **Be careful:** .get_text() can also provide you with texts that are not part of the aggregation (i.e., that have been filtered out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o9z7QGcZotk"
      },
      "outputs": [],
      "source": [
        "wikipedia.get_text(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIeAFZPxqVNp"
      },
      "source": [
        "#### 8.1 RegEx-Based Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol4tTp5WUE8k"
      },
      "source": [
        "It is technically not necessary to `compile` the regular expression. However, it often makes the code more readable and it is also advisable when using the same expression multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkfRfUX0E6vh"
      },
      "outputs": [],
      "source": [
        "wikipedia_cologne = wikipedia.get_text(0)\n",
        "search_term = 'city'\n",
        "lr = 25\n",
        "\n",
        "# Simple Solution\n",
        "# regex = re.compile(r'.{0,25}city\\b.{25}|city\\b.{0,25}', re.IGNORECASE)\n",
        "regex = re.compile(fr'.{{0,{lr}}}{search_term}\\b.{{{lr}}}|{search_term}\\b.{{0,{lr}}}', re.IGNORECASE)\n",
        "\n",
        "concordances = re.findall(regex, wikipedia_cologne)\n",
        "\n",
        "concordances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcr0ruEScxx9"
      },
      "source": [
        "The regular expression above looks very complicated because we're using f-strings (`f'{placeholder} in a text'`) in conjunction with a regular expression. As we need the `{}` characters in both cases, we need to \"escape\" them by doubling them whenever we want them to be actually there and not interpreted as f-string placeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDPGIL_lqRck"
      },
      "source": [
        "#### 8.2 Token-Based Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEydo1FJqdqq"
      },
      "source": [
        "Below we will define a `tokenize` function, which we will use repeatedly down the line. This simple regex tokenizer (`\\w+`), despite its simplicity, works quite well for English. Feel free to replace this function with something better and/or more powerful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak-pel7GGKbS"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  return re.findall(r'\\w+', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8A43pLVGajC"
      },
      "outputs": [],
      "source": [
        "tokenize('Hello world')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEWG63ow2j_t"
      },
      "source": [
        "As said above, this approach has its limits ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYp02bo32Y0r"
      },
      "outputs": [],
      "source": [
        "tokenize('this is a data-driven approach')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtbcp8bDCvD"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  return re.findall(r'[\\w-]+', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8gvVcHbyvNU"
      },
      "source": [
        "There are many ways to build a tokenizer. An alternative approach would be to use `\\S` (non-whitespace characters). However, this is sensitive for punctuation marks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzDTsD7NXGTM"
      },
      "source": [
        "In this variant, we are not differentiating between the left and right span."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhfYBNP9Gdiv"
      },
      "outputs": [],
      "source": [
        "wikipedia_cologne_tokenized = tokenize(wikipedia_cologne)\n",
        "search_word = 'city'\n",
        "lr = 4\n",
        "\n",
        "for id in range(len(wikipedia_cologne_tokenized)):\n",
        "  if wikipedia_cologne_tokenized[id] == search_word:\n",
        "    kwic = ' '.join(wikipedia_cologne_tokenized[id - lr : id + lr + 1])\n",
        "    \n",
        "    print(kwic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-EODl8POHV"
      },
      "source": [
        "We could have also used `enumerate` in this case. But, ultimately, as we need to work with the indices anyway, this comes primarily down to personal preference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5e2ygA8PEOn"
      },
      "outputs": [],
      "source": [
        "wikipedia_cologne_tokenized = tokenize(wikipedia_cologne)\n",
        "search_word = 'city'\n",
        "lr = 4\n",
        "\n",
        "for id, token in enumerate(wikipedia_cologne_tokenized):\n",
        "  if token == search_word:\n",
        "    kwic = ' '.join(wikipedia_cologne_tokenized[id - lr : id + lr + 1])\n",
        "    \n",
        "    print(kwic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5jC3INWXL1L"
      },
      "source": [
        "Here, we are creating two separate strings for the left and right span. These are then printed using `tabulate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDQyC4ZLH-HP"
      },
      "outputs": [],
      "source": [
        "search_word = 'city'\n",
        "lr = 4\n",
        "kwic = []\n",
        "\n",
        "for id in range(len(wikipedia_cologne_tokenized)):\n",
        "  if wikipedia_cologne_tokenized[id] == search_word:\n",
        "\n",
        "    l = ' '.join(wikipedia_cologne_tokenized[id - lr:id])\n",
        "    r = ' '.join(wikipedia_cologne_tokenized[id + 1: id + lr + 1])\n",
        "    kwic.append([l, search_word, r])\n",
        "\n",
        "print(tabulate(kwic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI56CNlzXV8z"
      },
      "source": [
        "It is very helpful to sort concordances. Given our approach above, we can sort either by the left or right context. We can use `itemgetter` to sort the list of lists based on a subkey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRHb0u8idPDJ"
      },
      "outputs": [],
      "source": [
        "kwic.sort(key=itemgetter(2))\n",
        "\n",
        "print(tabulate(kwic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifm5BisVseyt"
      },
      "source": [
        "### Exercise 9 – N-Grams\n",
        "**Note:** Number of N-Grams = Tokens + 1 - N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7JBw7IqnF2Z"
      },
      "outputs": [],
      "source": [
        "text = 'I really like Python, it is pretty awesome.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4oGT1HJyVK"
      },
      "source": [
        "#### 9.1 NLTK Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGWc-VyXJw5f"
      },
      "outputs": [],
      "source": [
        "def nltk_ngrams(text, n=3):\n",
        "  tokenized_text = tokenize(text)\n",
        "  ngrams = list(nltk.ngrams(tokenized_text, n))\n",
        "  return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WywLVUZjKaHx"
      },
      "outputs": [],
      "source": [
        "nltk_ngrams(text, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgL-zEApKuXI"
      },
      "source": [
        "#### 9.2 Plain Old Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEdyRRZjKv8v"
      },
      "outputs": [],
      "source": [
        "def ngrams_pop(text, n=3):\n",
        "  tokenized_text = tokenize(text)\n",
        "  no_of_ngrams = len(tokenized_text) + 1 - n\n",
        "  ngrams = []\n",
        "\n",
        "  for i in range(no_of_ngrams):\n",
        "    #print(i, tokenized_text[i:i+n])\n",
        "    ngrams.append(tokenized_text[i:i + n])\n",
        "\n",
        "  return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVXw0k1-Ljjb"
      },
      "outputs": [],
      "source": [
        "ngrams_pop(text, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqbyn44Si-Y"
      },
      "source": [
        "#### 9.3 ChatGPT Solution\n",
        "\n",
        "The code in the following two cells has been taken from ChatGPT, which was prompted \"Write a Python function that extracts n-grams from a given text.\" Then I followed up with: \"What would this look like for  text = 'I really like Python, it is pretty awesome.'\". This led to the usage example in cell two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WinbsgFpSn_Z"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz6emwnzSoQc"
      },
      "outputs": [],
      "source": [
        "text = 'I really like Python, it is pretty awesome.'\n",
        "tokens = text.split()\n",
        "ngrams = get_ngrams(tokens, 3)\n",
        "print(ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbitsVl5NDQF"
      },
      "source": [
        "### Exercise 10 – Frequency Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8s_xCXi_-Y"
      },
      "source": [
        "We're going to use the `wikipedia_cologne` text for this exercise again. The `tokenize` function is the one from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtOZ6yewi_Hl"
      },
      "outputs": [],
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)\n",
        "wikipedia_cologne = wikipedia.get_text(0)\n",
        "wikipedia_cologne_tokenized = tokenize(wikipedia_cologne)\n",
        "\n",
        "print(f'There are {len(wikipedia_cologne_tokenized)} tokens in wikipedia_cologne')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJmUfTeMZE3"
      },
      "source": [
        "#### 10.1 Counter Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-sLwsc-MomI"
      },
      "outputs": [],
      "source": [
        "Counter(wikipedia_cologne_tokenized).most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zy-2GtFhfDn"
      },
      "source": [
        "Let's visualize the frequencies ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNWgcuuLg9ua"
      },
      "outputs": [],
      "source": [
        "f = dict(Counter(wikipedia_cologne_tokenized).most_common(20))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "sns.barplot(x=list(f.keys()), y=list(f.values()), palette='Blues_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Vrlhgjvy6Q"
      },
      "source": [
        "Of course, often we are also interested in relative frequencies ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTI8sDcNv6BW"
      },
      "outputs": [],
      "source": [
        "def per_10k(abs_frequency, corpus_size):\n",
        "  return round(abs_frequency / corpus_size * 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iax0hsKao-x9"
      },
      "source": [
        "Down below, we will use this function again, but we will call it `relative_frequency`. We could also create a more general/abstract function that allows us to normalize given an arbitrary number (here `n`), e.g. per million words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0kIHnkln9mP"
      },
      "outputs": [],
      "source": [
        "def per_n(abs_frequency, corpus_size, n):\n",
        "  return round(abs_frequency / corpus_size * n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5mJ1ET7wbmN"
      },
      "source": [
        "**Note on Unpacking:** In the following cell we will use something called \"unpacking\". It allows us to *unpack* an item during iteration. \n",
        "\n",
        "Before we go on, here's an example. We're going to unpack a list of lists with people and their age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsPeHeojwpUN"
      },
      "outputs": [],
      "source": [
        "people = [\n",
        "    ['Person A', 20],\n",
        "    ['Person B', 30],\n",
        "]\n",
        "\n",
        "for name, age in people:\n",
        "  print(f'{name} is {age}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7RdGBCOwUwW"
      },
      "outputs": [],
      "source": [
        "f = dict(Counter(wikipedia_cologne_tokenized))\n",
        "\n",
        "# Alternatively, we could just do len(wikipedia_cologne_tokenized)\n",
        "corpus_size = sum(f.values())\n",
        "\n",
        "relative_frequencies = {}\n",
        "\n",
        "for w, abs_frequency in f.items():\n",
        "  relative_frequencies[w] = per_10k(abs_frequency, corpus_size)\n",
        "\n",
        "print_dict(relative_frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAuQ5Wzl-sCU"
      },
      "source": [
        "Have a look at the [*Frequency Distribution*](https://github.com/IngoKl/python-programming-for-linguists/blob/main/2021/exercises/Additional_Exercises_Frequency_Distribution.ipynb) notebook for an additional discussion of frequency analysis and frequency distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kv7q0G1LyRt"
      },
      "source": [
        "#### 10.2 NLTK Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuoQYwE4L57Z"
      },
      "outputs": [],
      "source": [
        "frequencies = nltk.probability.FreqDist(wikipedia_cologne_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGYdu_8VnSrp"
      },
      "outputs": [],
      "source": [
        "frequencies.pprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apj48hltMDzM"
      },
      "outputs": [],
      "source": [
        "frequencies['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEVj12yck_7f"
      },
      "source": [
        "NLTK's `FreqDist` has some very helpful features. For example, we can extract *Hapax Legomena* very easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71X-eprzk1wI"
      },
      "outputs": [],
      "source": [
        "frequencies.hapaxes()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNTntPAkZgYN"
      },
      "source": [
        "We can also easily plot `FreqDist` objects by calling the `.plot()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSPKk2KjMMVG"
      },
      "outputs": [],
      "source": [
        "frequencies.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0x5-WJvqphq"
      },
      "source": [
        "It's worthwhile to look at the documentation of libraries we are using. For example, looking at the [`FreqDist` documentation](https://www.nltk.org/api/nltk.probability.FreqDist.html), we can see that there's `tabulate` method available to us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMlRcKUtqiKb"
      },
      "outputs": [],
      "source": [
        "frequencies.tabulate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ4HteNVa--F"
      },
      "source": [
        "#### 10.3 spaCy Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTGc2UPGbAnh"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(wikipedia_cologne)\n",
        "\n",
        "frequencies = doc.count_by(spacy.attrs.IDS['ORTH'])\n",
        "\n",
        "print_dict(frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heLgCWHYZ7WQ"
      },
      "source": [
        "If we have the index of a given word (entry in the vocabulary), we can easily retrieve the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-oMpwA6bb7D"
      },
      "outputs": [],
      "source": [
        "doc.vocab[7425985699627899538].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke7zsq6EbjM2"
      },
      "outputs": [],
      "source": [
        "for vocab_index, count in frequencies.items():\n",
        "    human_readable = doc.vocab[vocab_index].text\n",
        "    \n",
        "    print(human_readable, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF9NL_sMl4jQ"
      },
      "source": [
        "### Exercise 11 – Computing Basic Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrFEDf1RtX3w"
      },
      "source": [
        "#### HUM19UK via TextDirectory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YnFv7TSubhQ5"
      },
      "source": [
        "We use `TextDirectory` to load the *HUM19UK corpus*. Then we are selecting a random sample of 10 texts and transform everything to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTnuAEKHtcuG"
      },
      "outputs": [],
      "source": [
        "hum19uk = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/hum19uk', autoload=True)\n",
        "hum19uk.filter_by_random_sampling(10)\n",
        "hum19uk.stage_transformation(['transformation_lowercase'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Oa3bveHtd-c"
      },
      "outputs": [],
      "source": [
        "hum19uk.transform_to_memory()\n",
        "hum19uk.print_aggregation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv6q6KKCcVE9"
      },
      "source": [
        "#### 11.1 Basic Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMyHIePjyLrd"
      },
      "source": [
        "Tokenizing in the `get_frequencies` function is convenient for us here. However, this will inevitable lead to us tokenizing some texts more than once. For the `get_frequencies` function, we are relying on the Counter approach from above – something you would not want to do in a real-life scenario in order to save time and resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI_0H0ulNlwL"
      },
      "outputs": [],
      "source": [
        "def get_frequencies(text):\n",
        "  tokenized_text = tokenize(text)\n",
        "  frequencies = Counter(tokenized_text)\n",
        "\n",
        "  return frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cDubBTx3Pm-"
      },
      "source": [
        "The `Counter` has a nice additional property. `Counter` objects will return 0 if the element is not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb2C7KfG3WKP"
      },
      "outputs": [],
      "source": [
        " test_text = 'The cat is black'\n",
        " f_cat = get_frequencies(test_text)['cat']\n",
        " f_dog = get_frequencies(test_text)['dog']\n",
        "\n",
        " f_cat, f_dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSohjZDeN3Mv"
      },
      "outputs": [],
      "source": [
        "def relative_frequency(abs_frequency, corpus_size):\n",
        "  return (abs_frequency / corpus_size) * 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig-_XAwZOI5u"
      },
      "outputs": [],
      "source": [
        "def frequency_across_text(search_term, texts):\n",
        "  frequency_list = []\n",
        "\n",
        "  for text in texts:\n",
        "    frequencies = get_frequencies(text)\n",
        "    frequency_list.append(frequencies[search_term])\n",
        "\n",
        "  return frequency_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJpqQ-l4etoG"
      },
      "source": [
        "Let's test this function with a very simple example. We want to get a list of frequencies for a given search term and a number of texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUfozHPheb5d"
      },
      "outputs": [],
      "source": [
        "texts = ['test test test', 'test test', 'test']\n",
        "frequency_across_text('test', texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmGW4KRBcFax"
      },
      "source": [
        "To normalize the frequency counts, we need the number of tokens in the corpus. We can get this number by getting the length (`len`) of the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1tvQaEeP1AK"
      },
      "outputs": [],
      "source": [
        "def frequency_across_text_relative(search_term, texts):\n",
        "  frequency_list = []\n",
        "\n",
        "  for text in texts:\n",
        "    frequencies = get_frequencies(text)\n",
        "    corpus_size = len(tokenize(text))\n",
        "\n",
        "    relative_frequency_of_search_term = relative_frequency(frequencies[search_term], corpus_size)\n",
        "    \n",
        "    frequency_list.append(relative_frequency_of_search_term)\n",
        "\n",
        "  return frequency_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POH6kKAWc7Xx"
      },
      "source": [
        "This list comprehension will generate a list of strings, each containing the text of one document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz149CLIO6Cu"
      },
      "outputs": [],
      "source": [
        "texts = [doc['transformed_text'] for doc in list(hum19uk.get_aggregation())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yq0RIlldBUO"
      },
      "source": [
        "We are now generating the frequencies for *shook* for all texts and storing them in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StPUHEgDOwzP"
      },
      "outputs": [],
      "source": [
        "frequencies_across_texts = frequency_across_text('shook', texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyNRqKAqQW70"
      },
      "outputs": [],
      "source": [
        "frequencies_across_texts_relative = frequency_across_text_relative('shook', texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGLjJDgXPZ2o"
      },
      "outputs": [],
      "source": [
        "statistics.mean(frequencies_across_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCHEMuWxPjZM"
      },
      "outputs": [],
      "source": [
        "statistics.stdev(frequencies_across_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWwc2oAzQbcs"
      },
      "outputs": [],
      "source": [
        "statistics.mean(frequencies_across_texts_relative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UF7c3rhQlh6"
      },
      "source": [
        "#### 11.2 Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chXwPVCGda8V"
      },
      "source": [
        "We typecast (force a new type) the list of tokens into a set. This will remove all duplicates and provide us with an unsorted list of all types. This, in NLP, would be considered to be the *vocabulary*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUOs8x5BQvQe"
      },
      "outputs": [],
      "source": [
        "text = hum19uk.aggregate_to_memory()\n",
        "tokenized_text = tokenize(text)\n",
        "vocabulary = set(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "166s91PjRLj2"
      },
      "outputs": [],
      "source": [
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRRsTbtk0Mhf"
      },
      "source": [
        "We could, but here we don't have to, turn this set into a list again. This way, we could order the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fby8RAt30MK0"
      },
      "outputs": [],
      "source": [
        "ordered_vocabulary = list(vocabulary)\n",
        "ordered_vocabulary.sort()\n",
        "ordered_vocabulary[20000:20010] # Getting a slice of types from the middle of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6JM3PEaQnaK"
      },
      "outputs": [],
      "source": [
        "# Initialize the frequency tables\n",
        "frequency_table_abs = {}\n",
        "frequency_table_rel = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_XaFfcfBSJ"
      },
      "source": [
        "We are looping over the vocabulary (all types in the corpus) and are adding the frequencies (both absolute and relative) to lists. Finally, after finishing a document, we are adding these lists to the frequency tables defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7rjEp2XRSoR"
      },
      "outputs": [],
      "source": [
        "for doc in hum19uk.get_aggregation():\n",
        "  doc_frequencies = get_frequencies(doc['transformed_text'])\n",
        "\n",
        "  doc_frequency_list_abs = []\n",
        "  doc_frequency_list_rel = []\n",
        "\n",
        "  for vocab in vocabulary:\n",
        "    doc_frequency_list_abs.append(doc_frequencies[vocab])\n",
        "    doc_frequency_list_rel.append(relative_frequency(doc_frequencies[vocab], doc['tokens']))\n",
        "\n",
        "  frequency_table_abs[doc['filename']] = doc_frequency_list_abs\n",
        "  frequency_table_rel[doc['filename']] = doc_frequency_list_rel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy1m9dIYd0Zv"
      },
      "source": [
        "**Absolute Frequencies**\n",
        "\n",
        "The key here is to use the `vocabulary` as the index. This will allow us to see the actual types in our table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB4HcnGHSdOB"
      },
      "outputs": [],
      "source": [
        "df_abs = pd.DataFrame(frequency_table_abs, index=vocabulary)\n",
        "df_abs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6bmuklXMfFq"
      },
      "source": [
        "Of course, we now also easily get things like the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNmjjN6STyoG"
      },
      "outputs": [],
      "source": [
        "df_abs.loc['the'].std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MoOT17Kd2S_"
      },
      "source": [
        "**Relative Frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhmgvd1JUD5T"
      },
      "outputs": [],
      "source": [
        "df_rel = pd.DataFrame(frequency_table_rel, index=vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljsOyt62UK7E"
      },
      "outputs": [],
      "source": [
        "df_rel.loc[['telegraph', 'the']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi088HxRd-En"
      },
      "source": [
        "We sort the `DataFrame` by its colums before plotting the frequencies for *telegraph*. Since in HUM19UK the files (and so the columns) have years as their names, this will provide us with a diachronic frequency plot.\n",
        "\n",
        "Of course, this is now based only on our sample of ten. Increase the sample size and run all cells above to get a fuller picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP-4sUulURz7"
      },
      "outputs": [],
      "source": [
        "df_rel.reindex(sorted(df_rel.columns), axis=1).loc['telegraph'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x2eW9EKeNyx"
      },
      "source": [
        "We can sum up the frequencies across texts for all words. Plotting these, sorted by the total, will result in a (more or less) Zipfian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxoPQh6YU-ic"
      },
      "outputs": [],
      "source": [
        "df_rel['total'] = df_rel.sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WefXCriaVF1d"
      },
      "outputs": [],
      "source": [
        "df_rel.sort_values(by='total', ascending=False)['total'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxMMMKCjhvd"
      },
      "source": [
        "Now that we have the information in a `DataFrame`, we can also easily export to other formats. For example, we could easily export our data to Excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH93ttwijNjR"
      },
      "outputs": [],
      "source": [
        "df_rel.to_excel('frequencies_relative.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYG5cZGAj8Yk"
      },
      "source": [
        "### Exercise 12 – Basic Collocation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVVVy7G6kQNU"
      },
      "source": [
        "#### Corpus / Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_FmWJinkBSa"
      },
      "outputs": [],
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)\n",
        "wikipedia.stage_transformation(['transformation_lowercase'])\n",
        "wikipedia.aggregate_to_memory()\n",
        "\n",
        "wikipedia_linguistics = tokenize(wikipedia.get_text(1))\n",
        "\n",
        "len(wikipedia_linguistics), wikipedia_linguistics[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562i7AuW5t2o"
      },
      "source": [
        "#### 12.1 NLTK Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKSQXMsl1sZ-"
      },
      "source": [
        "Of course, `nltk` provides us with a relatively straightforward solution. However, their solution, at least when following the default path, is not aligned with what we're used to in CL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH_zS6X21r41"
      },
      "outputs": [],
      "source": [
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(wikipedia_linguistics, window_size=3)\n",
        "\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpyuYGwI6yea"
      },
      "source": [
        "#### 12.2 Collocation from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENuY9vnStsSz"
      },
      "source": [
        "First, we need to define a function to calculate an MI score. We also need a function to \"check\" our search window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAYs8Xv_-7YV"
      },
      "outputs": [],
      "source": [
        "def mi_score(o11, r1, c1, n):\n",
        "  e11 = (r1 * c1) / n\n",
        "  mi = math.log2(o11 / e11)\n",
        "\n",
        "  return mi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myOjxBKo9QFp"
      },
      "outputs": [],
      "source": [
        "def in_window(tokens, node, candidate, window_size=2):\n",
        "  \n",
        "  in_window = 0\n",
        "  node_positions = [i for i, token in enumerate(tokens) if token == node]\n",
        "\n",
        "  for node_position in node_positions:\n",
        "    window = tokens[node_position - window_size: node_position + window_size + 1]\n",
        "    #print(window)\n",
        "    in_window += window.count(candidate)\n",
        "\n",
        "  return in_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VBpL3cgCWx4"
      },
      "outputs": [],
      "source": [
        "in_window(wikipedia_linguistics, 'language', 'human', window_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du4RihjCt4b-"
      },
      "source": [
        "Having these, we can start looking for collocates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOl_jVwdGSr_"
      },
      "outputs": [],
      "source": [
        "def collocates(tokens, node, window_size=1, min_freq=1):\n",
        "  vocabulary = set(tokens)\n",
        "  collocates = {}\n",
        "\n",
        "  n = len(tokens) # Tokens in the corpus; This will stay stable\n",
        "  \n",
        "  for w in vocabulary:\n",
        "    if w != node:\n",
        "      o11 = in_window(tokens, node, w, window_size=window_size) # Frequency of the candidate in the window\n",
        "      r1 = tokens.count(w) # Frequency of the candidate\n",
        "      c1 = tokens.count(node) # Frequency of the node\n",
        "      \n",
        "      if o11 >= min_freq:\n",
        "        collocates[w] = (o11, mi_score(o11, r1, c1, n))\n",
        "\n",
        "  return pd.DataFrame.from_dict(collocates, orient='index', columns=['Freq.', 'MI']).sort_values(by='MI', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loDFH2oV_EF4"
      },
      "outputs": [],
      "source": [
        "collocates(wikipedia_linguistics, 'language', window_size=2, min_freq=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFk4KYuO5NP"
      },
      "source": [
        "### Exercise 13 – NLTK Stemming, Lemmatization, and WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb0y6Osn1nTx"
      },
      "source": [
        "In order to be able to use [WordNet](https://wordnet.princeton.edu), we have to download the database(s) using NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rMvHR8a1n6_"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet') # \"Classic\" WordNet\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKAFiuWEf_L2"
      },
      "source": [
        "#### Stemming and Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WfVYCxf-Px"
      },
      "source": [
        "Here, we are initializing two stemmers and one lemmatizer. The lemmatizer, as the name suggests, is based on underlying WordNet data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrLxT5XnVl87"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP9fRqVN-K0q"
      },
      "source": [
        "Please note that there are more stemmers and lemmatizers in NLTK. An interesting one is, for example, the `SnowballStemmer`. *Snowball* is a stemming framework by Martin Porter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrHJXJf7Vtx_"
      },
      "outputs": [],
      "source": [
        "porter_stemmer.stem('connection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HaVo9n8WAGu"
      },
      "outputs": [],
      "source": [
        "lancaster_stemmer.stem('connection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_XZTDFNVieM"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer.lemmatize('connection')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_4xoIpWh6Xr"
      },
      "source": [
        "We can (should) also pass PoS tags to the `WordNetLemmatizer` to make it even better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfceMqYliWaj"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer.lemmatize('driving')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvblX_luh_HA"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer.lemmatize('driving', 'v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwIYvQ9a2Xpk"
      },
      "source": [
        "Now let's focus on the words from the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCwM9SO6WMWs"
      },
      "outputs": [],
      "source": [
        "words = ['connection', 'become', 'caring', 'are', 'women', 'driving']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEr_LR2JWN5J"
      },
      "outputs": [],
      "source": [
        "for word in words:\n",
        "  ps = porter_stemmer.stem(word)\n",
        "  ls = lancaster_stemmer.stem(word)\n",
        "  wl = wordnet_lemmatizer.lemmatize(word) # We could/should provide the PoS\n",
        "\n",
        "  print(f'{word} - {ps}  {ls}  {wl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-tCNGfgT83"
      },
      "source": [
        "As can be seen above, the three approaches lead to rather different results. The `LancasterStemmer` is the most aggressive but also the fastest of the three.\n",
        "\n",
        "We can use the magic `%%timeit` command to test how fast these stemmers/lemmatizers work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul5_Md6NWk8w"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "porter_stemmer.stem('become')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u4ERoh-Wxwn"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "lancaster_stemmer.stem('become')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STamfso4W49J"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "wordnet_lemmatizer.lemmatize('become')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MrKAxuzhV6M"
      },
      "source": [
        "If we take the \"best of 3\" metrics, we can clearly see that the, arguably, inferior `LancasterStemmer`can save us a lot of time if we had a very large corpus. \n",
        "\n",
        "Of course, the lemmatizer was even faster. However, the lemmatizer will only work well if we have data that works nicely with, in this case, *WordNet*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tgu4xeu3nwE"
      },
      "outputs": [],
      "source": [
        "porter_stemmer.stem('Tweets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhHIkJhShsYJ"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer.lemmatize('Tweets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE0N3oSSgq4z"
      },
      "source": [
        "#### WordNet Synsets\n",
        "Using [WordNet's](https://wordnet.princeton.edu/) synsets, we are now trying to find possible synonyms for *fantastic*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-hEwU_zXJUd"
      },
      "outputs": [],
      "source": [
        "search_term = 'fantastic'\n",
        "\n",
        "synonyms = []\n",
        "\n",
        "for synset in wordnet.synsets(search_term):\n",
        "  for name in synset.lemma_names():\n",
        "    synonyms.append(name)\n",
        "\n",
        "synonyms = set(synonyms)\n",
        "\n",
        "synonyms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6a71ydw2Dea"
      },
      "source": [
        "### Exercise 14 – spaCy Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6_0ijlGYGhl"
      },
      "outputs": [],
      "source": [
        "wikipedia = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/wikipedia', autoload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bama5FSyjsVy"
      },
      "source": [
        "For this exercise we are using the smallest (pre-made) model for English available. If you need betters results, you might want to use a larger [model](https://spacy.io/usage/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Ic9SF8YKDr"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(wikipedia.get_text(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84AWpaZ-Ici"
      },
      "source": [
        "If you want to try a more sophisticated and transformer-based model, try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gED_Rhbe-K9X"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-transformers\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy_transformers\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "doc = nlp(wikipedia.get_text(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZIo85pr8UD6"
      },
      "source": [
        "Having a spaCy document, we can also access the tokens within the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uuq7gpdf7-Zo"
      },
      "outputs": [],
      "source": [
        "token_zero = doc[0]\n",
        "token_zero.pos_ # pos_ is the coarse-grained part-of-speech; tag is the fine-grained part-of-speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmHiDSyj3zE"
      },
      "source": [
        "#### Sentence Segmentation\n",
        "\n",
        "spaCy also provides us with an easy way of segmenting sentences. The sentences are provided by a generator `doc.sents`. Here, we are printing the first five sentences of our `doc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJgbbMOKYvUH"
      },
      "outputs": [],
      "source": [
        "# We are turning the generator into a list so that we can slice [0:5] it\n",
        "for sent in list(doc.sents)[0:5]:\n",
        "  print(f'{sent}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRiRfShj9jP"
      },
      "source": [
        "#### Tagging / Annotation\n",
        "\n",
        "spaCy documents consist of tokens. Each token, given the default processing pipeline, also has a lemma, a PoS tag, and its dependencies attached to it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPiL6cbKY-Nb"
      },
      "outputs": [],
      "source": [
        "for token in doc[0:10]:\n",
        "  print(token.text, token.lemma_, token.tag_, token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOQZQHUylfiP"
      },
      "source": [
        "We can also loop over all of the named entities. The results here are not great, but this is due to the small model we are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8iYAEcXZQEz"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents[0:20]:\n",
        "  print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn--IUYXlpHD"
      },
      "source": [
        "#### Dependency Graph\n",
        "\n",
        "Remember that `doc.sents` is a generator. The `next` function will simply provide us with the next available elements.\n",
        "\n",
        "We could also use use something like `sentence = doc.sents[0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUp9Qki9Z-te"
      },
      "outputs": [],
      "source": [
        "sentence = next(doc.sents)\n",
        "\n",
        "# To make the plot more readable, you can increase the distance option\n",
        "displacy.render(sentence, style='dep', jupyter=True, options={'distance': 60})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovBYK7RO2GU2"
      },
      "source": [
        "### Exercise 15 – Parsing XML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhBnWOHQbq8P"
      },
      "outputs": [],
      "source": [
        "with open('python-programming-for-linguists/2020/data/xml/bnc_style.xml', 'r') as f:\n",
        "  xml = f.read()\n",
        "\n",
        "print(xml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYHDFoKacAAW"
      },
      "source": [
        "#### 15.1 RegEx-Based Approach\n",
        "\n",
        "Parsing XML (or HTML, or anything for that matter) manually is usually not a good idea. If possible, as you will see below, rely on established libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8coXLZC9b_6j"
      },
      "outputs": [],
      "source": [
        "def find_elements_re(xml, attribute, att_value):\n",
        "  regex = re.compile(fr'(<.*{attribute}=\"{att_value}\".*?>(.*)<\\/.*?>)')\n",
        "\n",
        "  xml_elements = re.findall(regex, xml)\n",
        "\n",
        "  return [element[1].strip() for element in xml_elements]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cwFCgC_b-Zw"
      },
      "outputs": [],
      "source": [
        "find_elements_re(xml, 'pos', 'VERB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_Dlb6JddEv"
      },
      "source": [
        "#### 15.2 Parsing Approach (using *LXML*)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VNYeCBedcMT"
      },
      "outputs": [],
      "source": [
        "def find_elements_lxml(xml, attribute, att_value):\n",
        "  tree = lxml.etree.parse(StringIO(xml))\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # findall support XPath (see below)\n",
        "  elements = root.findall(f\"w[@{attribute}='{att_value}']\")\n",
        "\n",
        "  for element in elements:\n",
        "    print(element.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfL5wCszfDbO"
      },
      "outputs": [],
      "source": [
        "find_elements_lxml(xml, 'pos', 'VERB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u-N6EqgeFOL"
      },
      "source": [
        "##### XPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS58s8MoeFBK"
      },
      "outputs": [],
      "source": [
        "tree = lxml.etree.parse('python-programming-for-linguists/2020/data/xml/xpath_example.xml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M919YiZsAsmD"
      },
      "source": [
        "Get *verbs* on *page one*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skbQgk__ePKz"
      },
      "outputs": [],
      "source": [
        "elements = tree.findall(f\"/page[@pg_nr='1']/s/w[@pos='verb']\")\n",
        "\n",
        "[element.text for element in elements]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bsw2xRBAxhF"
      },
      "source": [
        "Get the *first word* in the *second sentence* on *page two*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ534HeEenBp"
      },
      "outputs": [],
      "source": [
        "elements = tree.findall(f\"/page/[@pg_nr='2']/s[2]/w[1]\")\n",
        "\n",
        "[element.text for element in elements]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCdQPse-HG9O"
      },
      "source": [
        "### Exercise 16 – Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81R_XHeshdQr"
      },
      "source": [
        "The `requests` library allows us to easily retrieve websites. It allows us to use Python as an HTTP client, similarly to a browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4wKg9rUhYo3"
      },
      "outputs": [],
      "source": [
        "url = 'https://en.wikipedia.org/wiki/COVID-19_pandemic'\n",
        "response = requests.get(url)\n",
        "\n",
        "# HTTP Status Code; First 25 characters of content\n",
        "response.status_code, response.content[0:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4OQJTbDfTJx"
      },
      "source": [
        "#### 16.1 HTML and *BeautifulSoup* Parsing\n",
        "\n",
        "Using `BeautifulSoup`, we can parse HTML very similarly to how we parsed XML. We are going to get the tree and then navigate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuLml5O7fRLd"
      },
      "outputs": [],
      "source": [
        "def scrape_wikipedia(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  content = soup.find('div', {'id': 'bodyContent'}) # This is the \"container\" holding the main article content\n",
        "\n",
        "  #return content.text\n",
        "  return content.find_all('p') # We are looking for all p(aragraph) elements because they contain the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p8EWX3-ftQq"
      },
      "outputs": [],
      "source": [
        "scrape_wikipedia('https://en.wikipedia.org/wiki/COVID-19_pandemic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5RV-4sGQLI_"
      },
      "source": [
        "Let's build a slightly better version of the function that returns only text. We do this by going over the paragraphs and extracting just their text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McOu7lDWQKut"
      },
      "outputs": [],
      "source": [
        "def scrape_wikipedia(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  content = soup.find('div', {'id': 'bodyContent'})\n",
        "\n",
        "  text = ''\n",
        "\n",
        "  for p in content.find_all('p'):\n",
        "    text += p.text\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r5J1-XfQUzF"
      },
      "outputs": [],
      "source": [
        "scrape_wikipedia('https://en.wikipedia.org/wiki/COVID-19_pandemic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozSrHO97GTcP"
      },
      "source": [
        "Since we are parsing the HTML (similarly to how we used `LXML`), we could also, for example, get all *H2* headlines. This works exactly the same as with the `p` elements above.\n",
        "\n",
        "For this example, we are also doing the request \"manually\" again, not relying on the function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1aBcvtrGYz-"
      },
      "outputs": [],
      "source": [
        "html = requests.get('https://en.wikipedia.org/wiki/COVID-19_pandemic')\n",
        "soup = BeautifulSoup(html.content)\n",
        "\n",
        "h2_headlines = soup.find_all('h2') # This will get all H2 HTML elements\n",
        "\n",
        "[h2_headline.text for h2_headline in h2_headlines]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5aseJ4JgVn2"
      },
      "source": [
        "#### 16.2 jusText Approach\n",
        "\n",
        "In the *jusText* repository you can find a [description of the boilerplate cleaning algorithm](https://github.com/miso-belica/jusText/blob/dev/doc/algorithm.rst). This is important as you should always try to understand how external libraries, especially if they perform \"magic\", work and what assumptions they make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzcpMxXIgXX1"
      },
      "outputs": [],
      "source": [
        "def scrape_wikipedia_jt(url):\n",
        "  html = requests.get(url).content\n",
        "  paragraphs = justext.justext(html, justext.get_stoplist('English'))\n",
        "\n",
        "  text = []\n",
        "\n",
        "  for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "      text.append(paragraph.text)\n",
        "\n",
        "  # Combine the paragraphs into one string\n",
        "  text = ' '.join(text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw3CkSdng7tF"
      },
      "outputs": [],
      "source": [
        "scrape_wikipedia_jt('https://en.wikipedia.org/wiki/COVID-19_pandemic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Fc6JoSkXo9"
      },
      "source": [
        "When working with stoplists (stopwords), it's always a good idea to have a look at the list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2pTJuaTkSPK"
      },
      "outputs": [],
      "source": [
        "list(justext.get_stoplist('English'))[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmr79eLxY7-Y"
      },
      "source": [
        "### Exercise 17 – Putting Everything Together (Keyword Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sfZtt8uhait"
      },
      "source": [
        "#### 1. Compiling a Tiny Wikipedia Corpus (Target Corpus)\n",
        "First, we are compiling a tiny Wikipedia corpus using web scraping. We are going to get three Wikipedia articles using the functions from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKw4hrBmhc25"
      },
      "outputs": [],
      "source": [
        "article_urls = [\n",
        "                'https://en.wikipedia.org/wiki/Linguistics',\n",
        "                'https://en.wikipedia.org/wiki/Sociolinguistics',\n",
        "                'https://en.wikipedia.org/wiki/Language_change'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMLjr_HooPU"
      },
      "source": [
        "Since we want all articles in one document (as one string), we start with an empty string and add the content for each article to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzNk6hAhtjz"
      },
      "outputs": [],
      "source": [
        "wikipedia_corpus = ''\n",
        "\n",
        "for url in article_urls:\n",
        "  wikipedia_corpus += scrape_wikipedia_jt(url) + '\\n' # Adding a linebreak after each article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTkOErcXddl"
      },
      "outputs": [],
      "source": [
        "wikipedia_corpus[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8UTFVFtYUd1"
      },
      "source": [
        "We can also remove the typical Wikipedia references (e.g., [0]) using a regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOzkCEz0YI6v"
      },
      "outputs": [],
      "source": [
        "wikipedia_corpus = re.sub(r'\\[[0-9]*]', '', wikipedia_corpus)\n",
        "wikipedia_corpus[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RfB2BZpIzT"
      },
      "source": [
        "We are transforming the whole text (corpus) into lowercase; this reduces the amount of types. We are also generating a tokenized version (list) of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-xpNnmh8Xa"
      },
      "outputs": [],
      "source": [
        "wikipedia_corpus = wikipedia_corpus.lower()\n",
        "wikipedia_corpus_tokenized = tokenize(wikipedia_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e72WcgqkZJAP"
      },
      "outputs": [],
      "source": [
        "wikipedia_corpus_tokenized[0:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsnd9C3riBCe"
      },
      "source": [
        "#### 2. COCA Sampler (Reference Corpus)\n",
        "\n",
        "We are using the COCA sampler as our reference corpus. Since we transformed the target corpus (Wikipedia) to lowercase, we will do the same to the reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqMUwO1viC1c"
      },
      "outputs": [],
      "source": [
        "coca_sampler = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/coca', autoload=True)\n",
        "coca_sampler.stage_transformation(['transformation_lowercase'])\n",
        "\n",
        "reference_corpus = coca_sampler.aggregate_to_memory()\n",
        "reference_corpus_tokenized = tokenize(reference_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2SULa8aZa2Z"
      },
      "outputs": [],
      "source": [
        "reference_corpus_tokenized[0:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRX1ZaqiY72"
      },
      "source": [
        "#### 3. Frequency Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDBSl6GRp2Hz"
      },
      "source": [
        "As in Exercise 10, we are getting the vocabulary of both corpora. We are using the `set` trick again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwjaogeAiarc"
      },
      "outputs": [],
      "source": [
        "vocabulary = set(reference_corpus_tokenized + wikipedia_corpus_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMNfUK_qxzS"
      },
      "source": [
        "Now, again very similarly to Exercise 10, we can generate a frequency table. We are using `enumerate` to get numerical labels:\n",
        "\n",
        "* **Target/Wikipedia** = 0\n",
        "* **Reference/COCA** = 1\n",
        "\n",
        "In the following, remember that `get_frequencies` tokenizes our text. Hence we're not passing the tokenized versions of the corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzkzU4c0ikPL"
      },
      "outputs": [],
      "source": [
        "frequency_table = {}\n",
        "\n",
        "for i, corpus in enumerate([wikipedia_corpus, reference_corpus]):\n",
        "  frequency_list = []\n",
        "\n",
        "  corpus_frequencies = get_frequencies(corpus)\n",
        "\n",
        "  for vocab in vocabulary:\n",
        "    frequency_list.append(corpus_frequencies[vocab])\n",
        "\n",
        "  frequency_table[i] = frequency_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzgrYRIVM2l"
      },
      "source": [
        "Our comparative `frequency_table` now contains frequency information (absolute) for all words in the combined `vocabulary` for both corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPc9YkG5jZAs"
      },
      "outputs": [],
      "source": [
        "df_keyness = pd.DataFrame(frequency_table, index=vocabulary)\n",
        "\n",
        "# To make our lives easier, we will rename the columns\n",
        "df_keyness = df_keyness.rename(columns={0: 'Wikipedia', 1: 'COCA'})\n",
        "\n",
        "df_keyness.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXr2h4_IUTgW"
      },
      "source": [
        "#### Digression: Lambda / Anonymous Functions\n",
        "On the surface level, and we will not go any deeper, these are functions without a name. They are used when we only require a function for a short period of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZMOir81Udnu"
      },
      "outputs": [],
      "source": [
        "x = lambda a: a + 10\n",
        "x(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGR3yQa9jw0f"
      },
      "source": [
        "#### 4. Keyness Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGKdU6-5q_Vs"
      },
      "source": [
        "We are using *Kilgariff's Simple Math Parameter* as our keyness statistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI6woh-KjwGN"
      },
      "outputs": [],
      "source": [
        "def smp(f_word_c0, f_word_c1, cs0, cs1, k=100):\n",
        "  rel_f_word_c0 = relative_frequency(f_word_c0, cs0)\n",
        "  rel_f_word_c1 = relative_frequency(f_word_c1, cs1)\n",
        "\n",
        "  smp = (rel_f_word_c0 + k) / (rel_f_word_c1 + k)\n",
        "\n",
        "  return smp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P96sVO5Nr3qa"
      },
      "source": [
        "To get some intuition on the SMP, we can have a look at two equally large (1000 tokens) corpora. If the word appears 1000 times in the target and 100 times in the reference, the SMP will be, based on *k*, ten. The *k* parameter works almost as a filter. The lower you set the parameter, the more low-frequency items you will 'get'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ppu9LK5kcr4"
      },
      "outputs": [],
      "source": [
        "smp(1000, 100, 1000, 1000, k=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjA0mRCFlG6Z"
      },
      "outputs": [],
      "source": [
        "df_keyness.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE8Z5BDQsTcw"
      },
      "source": [
        "We can retrieve the corpus sizes by simple checking the length of the token lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4cYMgF6lQFa"
      },
      "outputs": [],
      "source": [
        "cs0 = len(wikipedia_corpus_tokenized)\n",
        "cs1 = len(reference_corpus_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ab5EwYVtb-C"
      },
      "source": [
        "We can calculate the SMP value for each row (word) by using `.apply` and a Lambda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkMsOUcikpr2"
      },
      "outputs": [],
      "source": [
        "df_keyness['SMP'] = df_keyness.apply(lambda row: smp(row[0], row[1], cs0, cs1), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUrx57HRle4n"
      },
      "outputs": [],
      "source": [
        "df_keyness.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4L5VnD0tUof"
      },
      "source": [
        "In order to get the actual keywords, we can sort the DataFrame by the newly created SMP value and a given cutoff (e.g., 1.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6s8um65lqZZ"
      },
      "outputs": [],
      "source": [
        "df_keyness[df_keyness['SMP'] > 1.2].sort_values('SMP', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gkUWmaNsiEc"
      },
      "source": [
        "#### Bonus: Stemmed Version\n",
        "\n",
        "As you can see, in the keyword list *language* and *languages*, for example, are listed as two keywords. We can use stemming to get a better (well, dependent on your RQ) result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iut_n5qtpRM"
      },
      "source": [
        "This, for the sake of readability and understandability, is just a redefinition of the functions from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz8bEzSytmPg"
      },
      "outputs": [],
      "source": [
        "def smp(f_word_c0, f_word_c1, cs0, cs1, k=100):\n",
        "  rel_f_word_c0 = relative_frequency(f_word_c0, cs0)\n",
        "  rel_f_word_c1 = relative_frequency(f_word_c1, cs1)\n",
        "\n",
        "  smp = (rel_f_word_c0 + k) / (rel_f_word_c1 + k)\n",
        "\n",
        "  return smp\n",
        "\n",
        "\n",
        "def scrape_wikipedia_jt(url):\n",
        "  html = requests.get(url)\n",
        "  paragraphs = justext.justext(html.content, justext.get_stoplist('English'))\n",
        "\n",
        "  text = []\n",
        "\n",
        "  for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "      text.append(paragraph.text)\n",
        "\n",
        "  # Combine the paragraphs into one string\n",
        "  text = ' '.join(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "  return re.findall(r'\\w+', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWMunzfu-ic"
      },
      "source": [
        "Since we are now stemming our corpus we already have tokenized versions of them. Hence, we do not need/want our `get_frequencies` function to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANCL41VVvH8F"
      },
      "outputs": [],
      "source": [
        "def get_frequencies_tokenized_text(tokenized_text):\n",
        "  frequencies = Counter(tokenized_text)\n",
        "\n",
        "  return frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUbfAXbWuA9r"
      },
      "source": [
        "We need a new function which stems a text (well, a list of tokens). This function takes in a list of tokens and constructs a new list of stemmed tokens using the `LancasterStemmer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tecwh2qqty5y"
      },
      "outputs": [],
      "source": [
        "def stem_tokenized_text(text):\n",
        "\n",
        "  lancaster_stemmer = LancasterStemmer()\n",
        "  tokens = []\n",
        "\n",
        "  for token in text:\n",
        "    tokens.append(lancaster_stemmer.stem(token))\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_6N9RB5d1mv"
      },
      "outputs": [],
      "source": [
        "stem_tokenized_text(['language', 'languages'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBlXVsneD6V"
      },
      "source": [
        "For our purpose, it doesn't really matter that these are not \"actual\" words. What's important is that they can now be treated as the same thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GcDjFFuuK5e"
      },
      "source": [
        "Of course, we could achieve the same thing using a list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFc6pK1SuUMC"
      },
      "outputs": [],
      "source": [
        "text = 'The cars were driving to through the night.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg7qdil9uKQc"
      },
      "outputs": [],
      "source": [
        "lancaster_stemmer = LancasterStemmer()\n",
        "[lancaster_stemmer.stem(token) for token in tokenize(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgT2kYCbswNV"
      },
      "outputs": [],
      "source": [
        "article_urls = [\n",
        "                'https://en.wikipedia.org/wiki/Linguistics',\n",
        "                'https://en.wikipedia.org/wiki/Sociolinguistics',\n",
        "                'https://en.wikipedia.org/wiki/Language_change'\n",
        "]\n",
        "\n",
        "wikipedia = ''\n",
        "\n",
        "for url in article_urls:\n",
        "  wikipedia += scrape_wikipedia_jt(url)\n",
        "\n",
        "wikipedia_corpus = wikipedia.lower()\n",
        "wikipedia_corpus_tokenized = tokenize(wikipedia_corpus)\n",
        "wikipedia_corpus_stemmed = stem_tokenized_text(wikipedia_corpus_tokenized)\n",
        "\n",
        "coca_sampler = textdirectory.TextDirectory(directory='python-programming-for-linguists/2020/data/corpora/coca', autoload=True)\n",
        "coca_sampler.stage_transformation(['transformation_lowercase'])\n",
        "\n",
        "reference_corpus = coca_sampler.aggregate_to_memory()\n",
        "reference_corpus_tokenized = tokenize(reference_corpus)\n",
        "reference_corpus_stemmed = stem_tokenized_text(reference_corpus_tokenized)\n",
        "\n",
        "# We need to generate a stemmed version of the vocabulary\n",
        "vocabulary = set(wikipedia_corpus_stemmed + reference_corpus_stemmed)\n",
        "\n",
        "frequency_table = {}\n",
        "\n",
        "for i, corpus in enumerate([wikipedia_corpus_stemmed, reference_corpus_stemmed]):\n",
        "  frequency_list = []\n",
        "\n",
        "  # We need to get the frequencies for the stemmed/tokenized version.\n",
        "  corpus_frequencies = get_frequencies_tokenized_text(corpus)\n",
        "\n",
        "  for vocab in vocabulary:\n",
        "    frequency_list.append(corpus_frequencies[vocab])\n",
        "\n",
        "  frequency_table[i] = frequency_list\n",
        "\n",
        "df_keyness = pd.DataFrame(frequency_table, index=vocabulary)\n",
        "\n",
        "# To make our lives easier, we will rename the columns\n",
        "df_keyness = df_keyness.rename(columns={0: 'Wikipedia', 1: 'COCA'})\n",
        "\n",
        "df_keyness['SMP'] = df_keyness.apply(lambda row : smp(row[0], row[1], cs0, cs1), axis=1)\n",
        "\n",
        "df_keyness[df_keyness['SMP'] > 1.5].sort_values('SMP', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngmss-RjwKjm"
      },
      "source": [
        "Of course this output is far from pretty (also due to using the relatively fast `LancasterStemmer`). However, it bins linguistic items which belong together.\n",
        "\n",
        "Also, please note that this approach does not only work for word frequencies. We could just as well, for example, count PoS tags and look for *keytags* instead of keywords."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iJwE7TYEo1_7",
        "pBzngIdcLEeZ",
        "Tb1qBmMkgz4L",
        "q-8lFCNwiEtp",
        "vmGV6kC8ebGF",
        "siURhumtutW6",
        "n_BfVcYGKLRw",
        "vTQVw3BqLtSI",
        "CIeAFZPxqVNp",
        "JDPGIL_lqRck",
        "ifm5BisVseyt",
        "SL4oGT1HJyVK",
        "WgL-zEApKuXI",
        "ikqbyn44Si-Y",
        "1kv7q0G1LyRt",
        "MZ4HteNVa--F",
        "qYG5cZGAj8Yk",
        "YDFk4KYuO5NP",
        "l6a71ydw2Dea",
        "ovBYK7RO2GU2",
        "Id18_qZ52OyO",
        "zmr79eLxY7-Y"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3a07b995cb36b494b18415e0955c16896c9f4d40d49bdfea70e60aad8810a43a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
