{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Additional_Exercises_Frequency_Distribution",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8LHZw5jxwmW"
      },
      "source": [
        "# Additional Exercises - Frequency Distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFTxNWfN8Vua"
      },
      "source": [
        "In this notebook (set of exercises) we will create a tool that, given a corpus of text files and a search term, is able to provide us with information about the frequency distribution of the term across the files in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHvM1cmJ8qd7"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This is a little bit of setup. First, we import necessary libraries. Of course, feel free to add libraries as needed! After, we clone the workshop repository and use the provided helper script to download a series of Sherlock Holmes short stories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTSVAtts8RdO"
      },
      "source": [
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "# Pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Counter for getting frequencies\n",
        "from collections import Counter\n",
        "\n",
        "# DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCqH7MeyxzRa"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/IngoKl/python-programming-for-linguists\n",
        "!cd python-programming-for-linguists/2021/data && sh download_sherlockholmes.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-txyxOP9G5s"
      },
      "source": [
        "## Step 1: Preparing the Data\n",
        "\n",
        "After running the `download_sherlockholmes.sh` script above, you will have 12 short stories (*The Adventures of Sherlock Holmes*) in the `python-programming-for-linguists/2021/data/corpora/holmes` folder.\n",
        "\n",
        "The goal of this first step is to read and prepare the data. Your goal will be to create the data structure below. \n",
        "\n",
        "Please note that there are other, better and more efficient, data structures to achieve the same goals. However, we are building a solution that mirrors practices in corpus linguistics without being too conscious of memory of computation limitations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nlXgAZq9GYH"
      },
      "source": [
        "corpus = [\n",
        "          {\n",
        "           'filename': 'bery.txt', \n",
        "           'text': '...', \n",
        "           'story_title': 'THE ADVENTURE OF THE BERYL CORONET', \n",
        "           'length': None, \n",
        "           'frequencies': {}\n",
        "          },\n",
        "]\n",
        "\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPGW5NIW-nGN"
      },
      "source": [
        "Obviously, your solution will create a list with more than one item. The `frequencies` dictionary as well as `length` can be empty for now. We will populate it in the next step. `text` is supposed to contain the actual text.\n",
        "\n",
        "If you want to, you can preprocess the text before adding it to `corpus`.\n",
        "\n",
        "The trickiest bit is getting the `story_title` from the file. Have a look at one of the actual text files and remember what you've learned about regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx13B3qyAD3q"
      },
      "source": [
        "def get_story_title(text):\n",
        "  # YOUR CODE\n",
        "  title = None\n",
        "\n",
        "  return title\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # YOUR CODE\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_17lprz_MFg"
      },
      "source": [
        "corpus = []\n",
        "files = Path('python-programming-for-linguists/2021/data/corpora/holmes').glob('*.txt')\n",
        "\n",
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qizNQCTDTSt"
      },
      "source": [
        "## Step 2: Getting the Frequencies\n",
        "\n",
        "You will need to generate frequency tables and add them to `corpus`. At the same time, you should populate `length` with the number of tokens in the document.\n",
        "\n",
        "This also means that you will have to tokenize the stories first. Remember that you can use `dict()` to turn a `Counter` object into a dictionary.\n",
        "\n",
        "Ultimately, `frequency`, for each story, should contain a structure like below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnZ_FDT8Dn4P"
      },
      "source": [
        "frequencies = {\n",
        "    'word_a': 42,\n",
        "    'word_b': 12,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5VrpJaVDufW"
      },
      "source": [
        "def tokenize(text):\n",
        "  # YOUR CODE\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ObMUj8gDy5f"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhScros9GJeW"
      },
      "source": [
        "## Step 3: Frequencies and Frequency Distribution\n",
        "\n",
        "Now you will need to write a function that takes a `corpus` as well as `search_term`. You will also need to account for both the absolute as well as the relative (per 1,000 tokens) frequencies.\n",
        "\n",
        "If you need to check whether something is in a dictionary, you can do the following: `if x in y`\n",
        "\n",
        "You will generate a frequency table for the search term that looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wclpClooGXZr"
      },
      "source": [
        "frequency_table = {\n",
        "    # Filename: (abs_frequency, rel_frequency_per_1000)\n",
        "    'story_title_a': (1, 2),\n",
        "    'story_title_b': (1, 2)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZZufRoQG_S1"
      },
      "source": [
        "def get_frequencies(corpus, search_term):\n",
        "  frequency_table = {}\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  return frequency_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzoGahWOHF5-"
      },
      "source": [
        "The following code is **provided for you**. You don't have to change anything here. Just need to make sure that you `get_frequencies` function works well with it. \n",
        "\n",
        "* We will nicely print the results\n",
        "* We will calculate a very basic dispersion statistic (Range_2)\n",
        "* We will plot the results using `seaborn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B77mZcOA8H_4"
      },
      "source": [
        "def plot_frequency_table(frequency_table, search_term):\n",
        "\n",
        "  df = pd.DataFrame(frequency_table).transpose()\n",
        "  df.columns = ['abs_frequency', 'rel_frequency']\n",
        "  df = df.sort_values('rel_frequency', ascending=False)\n",
        "\n",
        "  ax = sns.barplot(y=df.index, x='rel_frequency', data=df, color='#EF2D56')\n",
        "  ax.set_title(f'Frequency Distribution of {search_term} (per 1,000 Tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEOGDIi-HNTR"
      },
      "source": [
        "search_term = 'watson'\n",
        "\n",
        "parts_with_st = 0\n",
        "frequency_table = get_frequencies(corpus, search_term)\n",
        "\n",
        "print(f'Distribution of \"{search_term}\":\\n')\n",
        "for s in frequency_table:\n",
        "  \n",
        "  if frequency_table[s][0] > 0:\n",
        "    parts_with_st += 1\n",
        "\n",
        "  print(f'- {frequency_table[s][0]} ({round(frequency_table[s][1], 2)} per 1,000 tokens) in {s}')\n",
        "\n",
        "# Range_2\n",
        "range_2 = ( parts_with_st / len(frequency_table.keys()) ) * 100\n",
        "\n",
        "print(f'\\nThe Range_2 is: {round(range_2, 2)}%\\n')\n",
        "\n",
        "plot_frequency_table(frequency_table, search_term)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}